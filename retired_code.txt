int: nc = 3;

var 1..nc: wa;   var 1..nc: nt;  var 1..nc: sa;   var 1..nc: q;
var 1..nc: nsw;  var 1..nc: v;   var 1..nc: t;

constraint wa != nt;
constraint wa != sa;
constraint nt != sa;
constraint nt != q;
constraint sa != q;
constraint sa != nsw;
constraint sa != v;
constraint q != nsw;
constraint nsw != v;
solve satisfy;

output ["wa=\(wa)\t nt=\(nt)\t sa=\(sa)\n",
        "q=\(q)\t nsw=\(nsw)\t v=\(v)\n",
         "t=", show(t),  "\n"];








import io
from flask import Flask, jsonify, request
from flask_cors import CORS, cross_origin
import jobs
import docker
import tempfile


#-----------------------------v CONFIG v-----------------------------#


app = Flask(__name__)
app.config["JSONIFY_MIMETYPE"] = "application/json"
CORS(app, resources={r"/api/*": {"origins": "*"}})


#-----------------------------v AUX FUNCTIONS v-----------------------------#

"""def build_image():

    dockerfile_content = b'
    # Use a base image that includes MiniZinc
    FROM minizinc/minizinc:latest

    # Create a directory for your MiniZinc application
    WORKDIR /app

    # Copy your MiniZinc model file into the container
    COPY . .

    # Define environment variables for the model string and solver name
    ENV MODEL_STRING=""
    ENV SOLVER_NAME="gecode"

    # Define the command to run your MiniZinc application, using MODEL_STRING
    CMD ["minizinc", "--solver", "$SOLVER_NAME", "$MODEL_STRING"]'

    # Convert the Dockerfile content to a file-like object
    dockerfile_obj = io.BytesIO(dockerfile_content)

    # Create a Docker client
    docker_client = docker.from_env()

    # Build the Docker image from the Dockerfile string
    image, build_logs = docker_client.images.build(
        fileobj=dockerfile_obj,
        rm=True,  # Remove intermediate containers
        tag='minizinc-job-image',
    )

    # Print build logs
    for log_entry in build_logs:
        print(log_entry)"""

#-----------------------------v ROUTES v-----------------------------# 


@app.route("/api", methods=["GET"])
@cross_origin()
def get_api_data():
    return jsonify({"message": "This is the API data. Hello"})


@app.route("/api/solver", methods=["POST"])
@cross_origin()
def solver():
    model_string = request.get_json()["model_string"]
    solver_results = jobs.start_minizinc_job(model_string)
    print("RESPONSE: ", solver_results)
    response = jsonify(solver_results)
    return response


#-----------------------------v INIT v-----------------------------# 


if __name__ == "__main__":
    app.run(host="0.0.0.0", debug=True)







    try {
      // Create a new JSON object containing the MiniZinc model string.
      const jsonData = {
        model_string: modelString,
      };

      // Send a POST request to the /api/solver endpoint with the JSON object as the body of the request.
      const response = await fetch("api/solver", {
        method: "POST",
        headers: new Headers({
          "Content-Type": "application/json",
        }),
        body: JSON.stringify(jsonData),
      });

      // Check if the response was successful.
      if (response.status === 200) {
        // Parse the JSON response to extract the results of the MiniZinc solve.
        const json = await response.json();

        // Update the state variables with the results of the MiniZinc solve.
        setMessage(json.message);
      } else {
        // Handle the error.
        console.error("Error fetching data:", response.statusText);
      }
    } catch (error) {
      // Handle the error.
      console.error("Error fetching data:", error);
    }
  }







    apiVersion: apps/v1
kind: Deployment
metadata:
  name: rabbitmq
spec:
  replicas: 1
  selector:
    matchLabels:
      app: rabbitmq
  template:
    metadata:
      labels:
        app: rabbitmq
    spec:
      containers:
        - name: rabbitmq
          image: bitnami/rabbitmq:latest
          ports:
            - containerPort: 5672
          env:
            - name: RABBITMQ_USERNAME
              value: guest
            - name: RABBITMQ_PASSWORD
              value: guest










apiVersion: v1
kind: Service
metadata:
  name: rabbitmq
spec:
  selector:
    app: rabbitmq
  ports:
    - protocol: TCP
      port: 5672
      targetPort: 5672






# Use a base image that includes MiniZinc
FROM minizinc/minizinc:latest

# Create a directory for your MiniZinc application
WORKDIR /app

# Copy your MiniZinc model file into the container
COPY . .

# Define environment variables for the model string and solver name
ENV MODEL_STRING=""
ENV SOLVER_NAME="gecode"

# Define the command to run your MiniZinc application, using MODEL_STRING
CMD ["minizinc", "--solver", "$SOLVER_NAME", "$MODEL_STRING"]








import os
import tempfile
from kubernetes import client, config
import time
import logging
import uuid

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def start_minizinc_job(model_string, namespace="default"):
    try:
        # Load Kubernetes configuration
        config.load_incluster_config()

        # Create a unique job name
        job_name = f"minizinc-job-{int(time.time())}-{str(uuid.uuid4())[:8]}"

        logger.info(f"Starting Minizinc job with name: {job_name}")

        # Get the current working directory
        current_directory = os.path.dirname(__file__)

        # Create a temporary file with the Minizinc model content in the current directory
        temp_file = tempfile.NamedTemporaryFile(mode='w+', delete=False, suffix=".mzn", dir=current_directory)
        temp_file.write(model_string)
        temp_model_path = temp_file.name

        # Create Minizinc job
        minizinc_job = create_minizinc_job(job_name, model_string)
        batch_api = client.BatchV1Api()
        batch_api.create_namespaced_job(namespace=namespace, body=minizinc_job)

        # Wait for the Minizinc job to complete
        wait_for_job_completion(batch_api, job_name, namespace)

    except Exception as e:
        logger.error(f"An error occurred: {e}")

def create_minizinc_job(job_name, model_string):
    return client.V1Job(
        metadata=client.V1ObjectMeta(name=job_name),
        spec=client.V1JobSpec(
            template=client.V1PodTemplateSpec(
                spec=client.V1PodSpec(
                    containers=[
                        client.V1Container(
                            name="minizinc-containe r",
                            image="minizinc/minizinc:latest",
                            command=["/bin/sh", "-c"],
                            args=[
                                "echo 'Starting Minizinc commands' && echo \"$MODEL_STRING\" > model.mzn && minizinc model.mzn && echo 'Finished Minizinc command'",
                            ],
                            env=[
                                client.V1EnvVar(name="MODEL_STRING", value=model_string),
                            ],
                        )
                    ],
                    restart_policy="OnFailure",
                )
            )
        )
    )

def wait_for_job_completion(api, job_name, namespace):
    while True:
        logger.info(f"Checking status for Minizinc job: {job_name}")
        job_status = api.read_namespaced_job_status(name=job_name, namespace=namespace)
        if job_status.status.succeeded:
            logger.info("Minizinc job completed successfully.")
            break
        elif job_status.status.failed:
            logger.error("Minizinc job failed.")
            break
        else:
            logger.info("Waiting for the Minizinc job to complete...")
            time.sleep(2)



import pika
def main():
    print("STARTING RECEIVE.....")
    connection_params = pika.ConnectionParameters(
        'definition.default.svc.cluster.local', 
        5672, 
        '/', 
        pika.PlainCredentials('XXXX', 'XXXXX)
    )

    connection = pika.BlockingConnection(connection_params)
    channel = connection.channel()

    channel.queue_declare(queue='hello')


    def callback(ch, method, properties, body):
        print(f" [x] Received {body}")

    channel.basic_consume(queue='hello',
                        auto_ack=True,
                        on_message_callback=callback)

    print(' [*] Waiting for messages. To exit press CTRL+C')
    channel.start_consuming()

import io
from flask import Flask, jsonify, request
from flask_cors import CORS, cross_origin
import jobs
import docker
import tempfile










#!/usr/bin/env python
import os
import pika

def sendHelloWorld():
  rabbitmq_username = os.getenv("RABBITMQ_USERNAME")
  rabbitmq_password = os.getenv("RABBITMQ_PASSWORD")
  print(rabbitmq_username, flush=True)
  print(rabbitmq_password, flush=True)
  connection = pika.BlockingConnection(pika.ConnectionParameters(host='message-broker.default.svc.cluster.local', credentials=pika.PlainCredentials(rabbitmq_username, rabbitmq_password)))
  channel = connection.channel()

  channel.queue_declare(queue='hello')

  channel.basic_publish(exchange='', routing_key='hello', body='Hello World!')
  print(" [x] Sent 'Hello World!'", flush=True)
  connection.close()



#!/bin/bash
start_time=$(date +%s)
minikube start --cpus=4 --memory=6000

eval $(minikube docker-env)

docker build -t frontend -f services/frontend/Dockerfile services/frontend/
docker build -t api-gateway -f services/api-gateway/Dockerfile services/api-gateway/
docker build -t solver-handler -f services/solver-handler/Dockerfile services/solver-handler/

kubectl apply -f kubernetes-deployments/rabbitmq-operator.yaml
kubectl apply -f kubernetes-deployments/rabbitmq-definition.yaml

kubectl apply -f kubernetes-deployments/api-gateway-deployment.yaml
kubectl apply -f kubernetes-deployments/api-gateway-service.yaml

kubectl apply -f kubernetes-deployments/frontend-deployment.yaml
kubectl apply -f kubernetes-deployments/frontend-service.yaml

kubectl apply -f kubernetes-deployments/solver-handler-deployment.yaml
kubectl apply -f kubernetes-deployments/solver-handler-service.yaml

end_time=$(date +%s)
execution_time=$((end_time - start_time))
echo "Script execution time: $execution_time seconds"











#!/usr/bin/env python
import pika
import uuid
import os

class SolverHandlerQueue(object):
    print(f"API mq self-conn.", flush=True)
    def __init__(self):
        self.connection = pika.BlockingConnection(
            pika.ConnectionParameters(
                host='message-broker.default.svc.cluster.local', 
                    credentials=pika.PlainCredentials(
                        os.getenv("RABBITMQ_USERNAME"), os.getenv("RABBITMQ_PASSWORD"))))

        print(f"API mq con.chan.", flush=True)
        self.channel = self.connection.channel()
        print(f"API mq decl. exclu.", flush=True)
        result = self.channel.queue_declare(queue='', exclusive=True)
        self.callback_queue = result.method.queue

        self.channel.basic_consume(
            queue=self.callback_queue,
            on_message_callback=self.on_response,
            auto_ack=True)
        print(f"API mq basic cons. done", flush=True)
        self.response = None
        self.corr_id = None

    def on_response(self, ch, method, props, body):
        print(f"API mq on_response {body}", flush=True)
        if self.corr_id == props.correlation_id:
            self.response = body

    def call(self, data):
        print(f"A-G MQ handler call: {data}", flush=True)
        self.response = None
        self.corr_id = str(uuid.uuid4())
        print(f"A-G MQ atmpt. basic publish to {self.callback_queue}", flush=True)
        self.channel.basic_publish(
            exchange='',
            routing_key='solver_handler_rpc_queue',
            properties=pika.BasicProperties(
                reply_to=self.callback_queue,
                correlation_id=self.corr_id,
                content_type='application/json',
            ),
            body=data)
        print(f"A-G MQ handler wait for response", flush=True)
        self.connection.process_data_events(time_limit=None)
        print(f"A-G MQ handler got resp. : {self.response}", flush=True)
        return str(self.response)
















        #!/usr/bin/env python
import pika
import uuid
import os

class SolverPodQueue(object):
    def __init__(self, queue_name):
        self.queue_name = queue_name

        self.connection = pika.BlockingConnection(
            pika.ConnectionParameters(
                host='message-broker.default.svc.cluster.local',
                credentials=pika.PlainCredentials(
                    os.getenv("RABBITMQ_USERNAME"), os.getenv("RABBITMQ_PASSWORD"))))

        self.channel = self.connection.channel()
        result = self.channel.queue_declare(queue=self.queue_name)
        self.callback_queue = result.method.queue

        self.channel.basic_consume(
            queue=self.callback_queue,
            on_message_callback=self.on_response,
            auto_ack=True)

        self.response = None
        self.corr_id = None

    def on_response(self, ch, method, props, body):
        print(f"So-Ha mq on_response {body}", flush=True)
        if self.corr_id == props.correlation_id:
            self.response = body

    def call(self, data):
        print(f"So-Ha MQ handler call: {data}", flush=True)
        self.response = None
        self.corr_id = str(uuid.uuid4())
        print(f"So-Ha MQ handler publishing", flush=True)
        self.channel.basic_publish(
            exchange='',
            routing_key=self.queue_name,
            properties=pika.BasicProperties(
                reply_to=self.callback_queue,
                correlation_id=self.corr_id,
                content_type='application/json',
            ),
            body=data)
        print(f"So-Ha MQ post to {self.queue_name} w8 response from {self.callback_queue}", flush=True)
        self.connection.process_data_events(time_limit=None)
        print(f"So-Ha MQ handler returning response: {self.response}", flush=True)
        return str(self.response)









        #!/usr/bin/env python
import pika
import os
import sys

def on_request(ch, method, props, body):
    print(f" [.] solver-pod message consumed! Got and {body} attempting reply to: {os.getenv('JOB_NAME')}", flush=True)
    result = "returning something new"
    ch.basic_publish(exchange='',
        routing_key=os.getenv("JOB_NAME"),
        properties=pika.BasicProperties(correlation_id = props.correlation_id),
        body=result)
    print(f"So-Po main acking", flush=True)
    ch.basic_ack(delivery_tag=method.delivery_tag)
    sys.exit(0)
    


if __name__ == '__main__':
    connection = pika.BlockingConnection(
                    pika.ConnectionParameters(
                        host='message-broker.default.svc.cluster.local',
                        credentials=pika.PlainCredentials(
                            os.getenv("RABBITMQ_USERNAME"), os.getenv("RABBITMQ_PASSWORD"))))

    print(f"So-Po main atmpt. connecting", flush=True)
    channel = connection.channel()
    print(f"So-Po main connection established", flush=True)


    print(f"So-Po main declare queue", flush=True)
    channel.queue_declare(queue=os.getenv("JOB_NAME"))
    print(f"So-Po main queue declared", flush=True)


    channel.basic_qos(prefetch_count=1)
    print(f"So-Po main basic consume", flush=True)
    channel.basic_consume(os.getenv("JOB_NAME"), on_message_callback=on_request)

    print(f" [x] Solver-pod awaiting RPC requests, from queue {os.getenv('JOB_NAME')}", flush=True)
    channel.start_consuming()
    






    #!/usr/bin/env python
import pika
import os
import solvers
import messageQueue

def on_request(ch, method, props, body):
    print(f" [.] start_solver_job(): {body}", flush=True)
    job_name = solvers.start_solver_job(body)
    pod_messagequeue = messageQueue.SolverPodQueue(job_name)
    response = pod_messagequeue.call(body)
    print(f" [.] response = {response}.. publishing..", flush=True)

    # Respone to handler
    ch.basic_publish(exchange='',
        routing_key=props.reply_to,
        properties=pika.BasicProperties(correlation_id = props.correlation_id),
        body=str(response))
    print(f"So-Ha main acking", flush=True)
    ch.basic_ack(delivery_tag=method.delivery_tag)



if __name__ == '__main__':
    connection = pika.BlockingConnection(
                pika.ConnectionParameters(
                    host='message-broker.default.svc.cluster.local', 
                        credentials=pika.PlainCredentials(
                            os.getenv("RABBITMQ_USERNAME"), os.getenv("RABBITMQ_PASSWORD"))))

    print(f"So-Ha main atmpt. connecting", flush=True)
    channel = connection.channel()
    print(f"So-Ha main declare queue", flush=True)
    channel.queue_declare(queue='solver_handler_rpc_queue', durable=True)
    channel.basic_qos(prefetch_count=1)
    print(f"So-Ha main basic consume", flush=True)
    channel.basic_consume(queue='solver_handler_rpc_queue', on_message_callback=on_request)

    print(" [x] Solver-handler awaiting RPC requests", flush=True)
    channel.start_consuming()







FROM python:latest

#ENV rabbitmqusername=$rabbitmqusername
#ENV rabbitmqpassword=$rabbitmqpassword

WORKDIR /app

COPY requirements.txt .

RUN pip install -r requirements.txt

COPY . .

CMD ["python", "src/main.py"]








features = {}

    # Extract constants
    constant_pattern = re.compile(r'\bint\s*:\s*(\w+)\s*=\s*([-\d]+)\s*;')
    constants = constant_pattern.findall(minizinc_content)

    features['num_constants'] = len(constants)

    # Extract variables
    var_pattern = re.compile(r'\bvar\s+(\S+)\s*:\s*(.*?)\s*;')
    variables = var_pattern.findall(minizinc_content)

    features['num_variables'] = len(variables)

    # Extract constraints with types
    constraint_pattern = re.compile(r'\bconstraint\s+(.*?)\s*;')
    constraints = constraint_pattern.findall(minizinc_content)

    constraint_type_pattern = re.compile(r'([<>!=]+)')
    constraint_types = [constraint_type_pattern.search(constraint).group(1) for constraint in constraints]

    # Count occurrences of each constraint type
    constraint_type_counts = {ctype: constraint_types.count(ctype) for ctype in set(constraint_types)}

    features['num_constraints'] = len(constraints)
    features['constraint_type_counts'] = constraint_type_counts

    # Extract solve type
    solve_pattern = re.compile(r'\bsolve\s+(.*?);')
    solve_match = solve_pattern.search(minizinc_content)

    if solve_match:
        features['solve_type'] = solve_match.group(1)
    else:
        features['solve_type'] = None

    return features



    class RequestQueue(object):
    def __init__(self, identifier):
        self.connection = pika.BlockingConnection(
            pika.ConnectionParameters(
                host='message-broker.default.svc.cluster.local', 
                    credentials=pika.PlainCredentials(
                        os.getenv("RABBITMQ_USERNAME"), os.getenv("RABBITMQ_PASSWORD"))))
        
        self.identifier = identifier
        self.res_queue = f'api-in-queue-{self.identifier}'
        self.channel = self.connection.channel()
        self.channel.queue_declare(queue='api-out-queue')
        self.channel.queue_declare(queue=f'api-in-queue-{self.identifier}')

    def publish(self, data):
        print("SENDING:::", flush=True)
        json_data = json.dumps(data)
        self.channel.basic_publish(exchange='', routing_key='request-queue', body=json_data)

    
    def consume(self):
        self.response = None
        def callback(ch, method, properties, body):
            print(f"Result recieved: {body}", flush=True)
            decoded_body = body.decode("utf-8")
            self.response = decoded_body
            ch.stop_consuming()
            ch.queue_delete(queue=self.res_queue)
            
        self.channel.basic_consume(queue=self.res_queue, on_message_callback=callback, auto_ack=True)
        self.channel.start_consuming()
        self.connection.close()
        return self.response








def consume_from_dynamic_queue(channel, identifier):

        result_queue_name = f"result-queue-{identifier}"
        print("RQN ", result_queue_name)

        def callback_dynamic(ch, method, properties, body):
            decoded_body = body.decode("utf-8")

            print(f"Dynamic queue: {result_queue_name} received: {decoded_body}", flush=True)

            ch.basic_publish(exchange='', routing_key=f"api-queue-{identifier}", body=decoded_body)
            ch.stop_consuming()
            ch.queue_delete(queue=result_queue_name)

        channel.queue_declare(queue=result_queue_name)
        channel.basic_consume(queue=result_queue_name, on_message_callback=callback_dynamic, auto_ack=True)

        print(f"Starting Consume from dynamic queue ({result_queue_name})..", flush=True)

        channel.start_consuming()