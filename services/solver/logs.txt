* 
* ==> Audit <==
* |--------------|-------------------|----------|------|---------|----------------------|----------------------|
|   Command    |       Args        | Profile  | User | Version |      Start Time      |       End Time       |
|--------------|-------------------|----------|------|---------|----------------------|----------------------|
| start        |                   | minikube | hmbl | v1.31.2 | 21 Aug 23 08:29 CEST |                      |
| start        | --driver docker   | minikube | hmbl | v1.31.2 | 21 Aug 23 08:29 CEST |                      |
| start        | --driver docker   | minikube | hmbl | v1.31.2 | 21 Aug 23 08:29 CEST | 21 Aug 23 08:31 CEST |
| update-check |                   | minikube | hmbl | v1.31.2 | 22 Aug 23 09:34 CEST | 22 Aug 23 09:34 CEST |
| update-check |                   | minikube | hmbl | v1.31.2 | 08 Sep 23 10:32 CEST | 08 Sep 23 10:32 CEST |
| start        |                   | minikube | hmbl | v1.31.2 | 05 Oct 23 15:00 CEST |                      |
| start        |                   | minikube | hmbl | v1.31.2 | 05 Oct 23 15:07 CEST | 05 Oct 23 15:07 CEST |
| kubectl      | -- get pods       | minikube | hmbl | v1.31.2 | 05 Oct 23 15:08 CEST | 05 Oct 23 15:08 CEST |
| kubectl      | -- get pods       | minikube | hmbl | v1.31.2 | 05 Oct 23 15:08 CEST | 05 Oct 23 15:08 CEST |
| update-check |                   | minikube | hmbl | v1.31.2 | 05 Oct 23 16:42 CEST | 05 Oct 23 16:42 CEST |
| update-check |                   | minikube | hmbl | v1.31.2 | 02 Nov 23 11:46 CET  | 02 Nov 23 11:46 CET  |
| start        |                   | minikube | hmbl | v1.31.2 | 02 Nov 23 11:52 CET  | 02 Nov 23 11:52 CET  |
| ip           |                   | minikube | hmbl | v1.31.2 | 02 Nov 23 11:55 CET  | 02 Nov 23 11:55 CET  |
| tunnel       |                   | minikube | hmbl | v1.31.2 | 02 Nov 23 12:16 CET  | 02 Nov 23 12:17 CET  |
| tunnel       |                   | minikube | hmbl | v1.31.2 | 02 Nov 23 12:17 CET  | 02 Nov 23 12:18 CET  |
| ip           |                   | minikube | hmbl | v1.31.2 | 02 Nov 23 12:18 CET  | 02 Nov 23 12:18 CET  |
| tunnel       |                   | minikube | hmbl | v1.31.2 | 02 Nov 23 12:18 CET  | 02 Nov 23 12:23 CET  |
| ip           |                   | minikube | hmbl | v1.31.2 | 02 Nov 23 12:22 CET  | 02 Nov 23 12:22 CET  |
| service      |                   | minikube | hmbl | v1.31.2 | 02 Nov 23 12:26 CET  |                      |
| service      | --all             | minikube | hmbl | v1.31.2 | 02 Nov 23 12:26 CET  |                      |
| service      | my-frontend --url | minikube | hmbl | v1.31.2 | 02 Nov 23 12:28 CET  |                      |
|--------------|-------------------|----------|------|---------|----------------------|----------------------|

* 
* ==> Last Start <==
* Log file created at: 2023/11/02 11:52:05
Running on machine: hmbl
Binary: Built with gc go1.20.7 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I1102 11:52:05.375229   15045 out.go:296] Setting OutFile to fd 1 ...
I1102 11:52:05.375303   15045 out.go:348] isatty.IsTerminal(1) = true
I1102 11:52:05.375305   15045 out.go:309] Setting ErrFile to fd 2...
I1102 11:52:05.375308   15045 out.go:348] isatty.IsTerminal(2) = true
I1102 11:52:05.375606   15045 root.go:338] Updating PATH: /home/hmbl/.minikube/bin
W1102 11:52:05.375735   15045 root.go:314] Error reading config file at /home/hmbl/.minikube/config/config.json: open /home/hmbl/.minikube/config/config.json: no such file or directory
I1102 11:52:05.376060   15045 out.go:303] Setting JSON to false
I1102 11:52:05.382417   15045 start.go:128] hostinfo: {"hostname":"hmbl","uptime":4279,"bootTime":1698918046,"procs":352,"os":"linux","platform":"debian","platformFamily":"debian","platformVersion":"12.2","kernelVersion":"6.1.0-13-amd64","kernelArch":"x86_64","virtualizationSystem":"kvm","virtualizationRole":"host","hostId":"b08de043-31af-4175-acfa-e9dac0b89cfb"}
I1102 11:52:05.382464   15045 start.go:138] virtualization: kvm host
I1102 11:52:05.384487   15045 out.go:177] üòÑ  minikube v1.31.2 on Debian 12.2
I1102 11:52:05.387974   15045 notify.go:220] Checking for updates...
I1102 11:52:05.388453   15045 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.27.4
I1102 11:52:05.389132   15045 driver.go:373] Setting default libvirt URI to qemu:///system
I1102 11:52:05.405070   15045 docker.go:121] docker version: linux-24.0.7:Docker Engine - Community
I1102 11:52:05.405135   15045 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1102 11:52:05.439133   15045 info.go:266] docker info: {ID:5ce85dda-3277-437d-b61f-d65724208581 Containers:67 ContainersRunning:0 ContainersPaused:0 ContainersStopped:67 Images:66 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:23 OomKillDisable:false NGoroutines:35 SystemTime:2023-11-02 11:52:05.433881276 +0100 CET LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:6.1.0-13-amd64 OperatingSystem:Debian GNU/Linux 12 (bookworm) OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:16 MemTotal:16710893568 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:hmbl Labels:[] ExperimentalBuild:false ServerVersion:24.0.7 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:61f9fd88f79f081d64d6fa3bb1a0dc71ec870523 Expected:61f9fd88f79f081d64d6fa3bb1a0dc71ec870523} RuncCommit:{ID:v1.1.9-0-gccaecfc Expected:v1.1.9-0-gccaecfc} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=apparmor name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.11.2] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.21.0]] Warnings:<nil>}}
I1102 11:52:05.439188   15045 docker.go:294] overlay module found
I1102 11:52:05.440834   15045 out.go:177] ‚ú®  Using the docker driver based on existing profile
I1102 11:52:05.442274   15045 start.go:298] selected driver: docker
I1102 11:52:05.442277   15045 start.go:902] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 Memory:3900 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.27.4 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.27.4 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/hmbl:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0}
I1102 11:52:05.442319   15045 start.go:913] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1102 11:52:05.442371   15045 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1102 11:52:05.474629   15045 info.go:266] docker info: {ID:5ce85dda-3277-437d-b61f-d65724208581 Containers:67 ContainersRunning:0 ContainersPaused:0 ContainersStopped:67 Images:66 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:23 OomKillDisable:false NGoroutines:35 SystemTime:2023-11-02 11:52:05.469527417 +0100 CET LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:6.1.0-13-amd64 OperatingSystem:Debian GNU/Linux 12 (bookworm) OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:16 MemTotal:16710893568 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:hmbl Labels:[] ExperimentalBuild:false ServerVersion:24.0.7 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:61f9fd88f79f081d64d6fa3bb1a0dc71ec870523 Expected:61f9fd88f79f081d64d6fa3bb1a0dc71ec870523} RuncCommit:{ID:v1.1.9-0-gccaecfc Expected:v1.1.9-0-gccaecfc} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=apparmor name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.11.2] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.21.0]] Warnings:<nil>}}
I1102 11:52:05.475284   15045 cni.go:84] Creating CNI manager for ""
I1102 11:52:05.475319   15045 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1102 11:52:05.475325   15045 start_flags.go:319] config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 Memory:3900 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.27.4 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.27.4 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/hmbl:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0}
I1102 11:52:05.477124   15045 out.go:177] üëç  Starting control plane node minikube in cluster minikube
I1102 11:52:05.478562   15045 cache.go:122] Beginning downloading kic base image for docker with docker
I1102 11:52:05.480012   15045 out.go:177] üöú  Pulling base image ...
I1102 11:52:05.481551   15045 preload.go:132] Checking if preload exists for k8s version v1.27.4 and runtime docker
I1102 11:52:05.481577   15045 preload.go:148] Found local preload: /home/hmbl/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.27.4-docker-overlay2-amd64.tar.lz4
I1102 11:52:05.481584   15045 cache.go:57] Caching tarball of preloaded images
I1102 11:52:05.481613   15045 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 in local docker daemon
I1102 11:52:05.481659   15045 preload.go:174] Found /home/hmbl/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.27.4-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I1102 11:52:05.481664   15045 cache.go:60] Finished verifying existence of preloaded tar for  v1.27.4 on docker
I1102 11:52:05.481724   15045 profile.go:148] Saving config to /home/hmbl/.minikube/profiles/minikube/config.json ...
I1102 11:52:05.497437   15045 image.go:83] Found gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 in local docker daemon, skipping pull
I1102 11:52:05.497447   15045 cache.go:145] gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 exists in daemon, skipping load
I1102 11:52:05.497457   15045 cache.go:195] Successfully downloaded all kic artifacts
I1102 11:52:05.497483   15045 start.go:365] acquiring machines lock for minikube: {Name:mk7bf83ddc6346c014e1e7e509ad9f6981b6ca17 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1102 11:52:05.497564   15045 start.go:369] acquired machines lock for "minikube" in 64.797¬µs
I1102 11:52:05.497578   15045 start.go:96] Skipping create...Using existing machine configuration
I1102 11:52:05.497583   15045 fix.go:54] fixHost starting: 
I1102 11:52:05.497743   15045 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1102 11:52:05.507800   15045 fix.go:102] recreateIfNeeded on minikube: state=Stopped err=<nil>
W1102 11:52:05.507811   15045 fix.go:128] unexpected machine state, will restart: <nil>
I1102 11:52:05.509607   15045 out.go:177] üîÑ  Restarting existing docker container for "minikube" ...
I1102 11:52:05.511047   15045 cli_runner.go:164] Run: docker start minikube
I1102 11:52:05.821776   15045 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1102 11:52:05.831190   15045 kic.go:426] container "minikube" state is running.
I1102 11:52:05.831431   15045 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1102 11:52:05.840228   15045 profile.go:148] Saving config to /home/hmbl/.minikube/profiles/minikube/config.json ...
I1102 11:52:05.840326   15045 machine.go:88] provisioning docker machine ...
I1102 11:52:05.840334   15045 ubuntu.go:169] provisioning hostname "minikube"
I1102 11:52:05.840359   15045 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1102 11:52:05.850369   15045 main.go:141] libmachine: Using SSH client type: native
I1102 11:52:05.850790   15045 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x80f160] 0x812200 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I1102 11:52:05.850796   15045 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I1102 11:52:05.851381   15045 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: read tcp 127.0.0.1:42506->127.0.0.1:32772: read: connection reset by peer
I1102 11:52:08.980538   15045 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I1102 11:52:08.980591   15045 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1102 11:52:08.991267   15045 main.go:141] libmachine: Using SSH client type: native
I1102 11:52:08.991948   15045 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x80f160] 0x812200 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I1102 11:52:08.991957   15045 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I1102 11:52:09.103178   15045 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1102 11:52:09.103190   15045 ubuntu.go:175] set auth options {CertDir:/home/hmbl/.minikube CaCertPath:/home/hmbl/.minikube/certs/ca.pem CaPrivateKeyPath:/home/hmbl/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/hmbl/.minikube/machines/server.pem ServerKeyPath:/home/hmbl/.minikube/machines/server-key.pem ClientKeyPath:/home/hmbl/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/hmbl/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/hmbl/.minikube}
I1102 11:52:09.103206   15045 ubuntu.go:177] setting up certificates
I1102 11:52:09.103212   15045 provision.go:83] configureAuth start
I1102 11:52:09.103244   15045 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1102 11:52:09.112466   15045 provision.go:138] copyHostCerts
I1102 11:52:09.112892   15045 exec_runner.go:144] found /home/hmbl/.minikube/ca.pem, removing ...
I1102 11:52:09.112899   15045 exec_runner.go:203] rm: /home/hmbl/.minikube/ca.pem
I1102 11:52:09.112935   15045 exec_runner.go:151] cp: /home/hmbl/.minikube/certs/ca.pem --> /home/hmbl/.minikube/ca.pem (1074 bytes)
I1102 11:52:09.113138   15045 exec_runner.go:144] found /home/hmbl/.minikube/cert.pem, removing ...
I1102 11:52:09.113140   15045 exec_runner.go:203] rm: /home/hmbl/.minikube/cert.pem
I1102 11:52:09.113154   15045 exec_runner.go:151] cp: /home/hmbl/.minikube/certs/cert.pem --> /home/hmbl/.minikube/cert.pem (1115 bytes)
I1102 11:52:09.113257   15045 exec_runner.go:144] found /home/hmbl/.minikube/key.pem, removing ...
I1102 11:52:09.113259   15045 exec_runner.go:203] rm: /home/hmbl/.minikube/key.pem
I1102 11:52:09.113273   15045 exec_runner.go:151] cp: /home/hmbl/.minikube/certs/key.pem --> /home/hmbl/.minikube/key.pem (1679 bytes)
I1102 11:52:09.113368   15045 provision.go:112] generating server cert: /home/hmbl/.minikube/machines/server.pem ca-key=/home/hmbl/.minikube/certs/ca.pem private-key=/home/hmbl/.minikube/certs/ca-key.pem org=hmbl.minikube san=[192.168.49.2 127.0.0.1 localhost 127.0.0.1 minikube minikube]
I1102 11:52:09.203348   15045 provision.go:172] copyRemoteCerts
I1102 11:52:09.203383   15045 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I1102 11:52:09.203404   15045 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1102 11:52:09.212565   15045 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/hmbl/.minikube/machines/minikube/id_rsa Username:docker}
I1102 11:52:09.294275   15045 ssh_runner.go:362] scp /home/hmbl/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1074 bytes)
I1102 11:52:09.310219   15045 ssh_runner.go:362] scp /home/hmbl/.minikube/machines/server.pem --> /etc/docker/server.pem (1196 bytes)
I1102 11:52:09.324584   15045 ssh_runner.go:362] scp /home/hmbl/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I1102 11:52:09.339160   15045 provision.go:86] duration metric: configureAuth took 235.941117ms
I1102 11:52:09.339169   15045 ubuntu.go:193] setting minikube options for container-runtime
I1102 11:52:09.339272   15045 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.27.4
I1102 11:52:09.339299   15045 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1102 11:52:09.349073   15045 main.go:141] libmachine: Using SSH client type: native
I1102 11:52:09.349323   15045 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x80f160] 0x812200 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I1102 11:52:09.349327   15045 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I1102 11:52:09.459876   15045 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I1102 11:52:09.459885   15045 ubuntu.go:71] root file system type: overlay
I1102 11:52:09.459949   15045 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I1102 11:52:09.459984   15045 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1102 11:52:09.469720   15045 main.go:141] libmachine: Using SSH client type: native
I1102 11:52:09.469981   15045 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x80f160] 0x812200 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I1102 11:52:09.470025   15045 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I1102 11:52:09.586577   15045 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I1102 11:52:09.586616   15045 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1102 11:52:09.595769   15045 main.go:141] libmachine: Using SSH client type: native
I1102 11:52:09.596050   15045 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x80f160] 0x812200 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I1102 11:52:09.596059   15045 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I1102 11:52:09.711245   15045 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1102 11:52:09.711255   15045 machine.go:91] provisioned docker machine in 3.870924141s
I1102 11:52:09.711260   15045 start.go:300] post-start starting for "minikube" (driver="docker")
I1102 11:52:09.711266   15045 start.go:329] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I1102 11:52:09.711305   15045 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I1102 11:52:09.711326   15045 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1102 11:52:09.720374   15045 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/hmbl/.minikube/machines/minikube/id_rsa Username:docker}
I1102 11:52:09.803001   15045 ssh_runner.go:195] Run: cat /etc/os-release
I1102 11:52:09.804670   15045 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I1102 11:52:09.804683   15045 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I1102 11:52:09.804689   15045 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I1102 11:52:09.804692   15045 info.go:137] Remote host: Ubuntu 22.04.2 LTS
I1102 11:52:09.804697   15045 filesync.go:126] Scanning /home/hmbl/.minikube/addons for local assets ...
I1102 11:52:09.804892   15045 filesync.go:126] Scanning /home/hmbl/.minikube/files for local assets ...
I1102 11:52:09.804975   15045 start.go:303] post-start completed in 93.711732ms
I1102 11:52:09.804997   15045 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I1102 11:52:09.805016   15045 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1102 11:52:09.815241   15045 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/hmbl/.minikube/machines/minikube/id_rsa Username:docker}
I1102 11:52:09.892422   15045 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I1102 11:52:09.894814   15045 fix.go:56] fixHost completed within 4.397230709s
I1102 11:52:09.894820   15045 start.go:83] releasing machines lock for "minikube", held for 4.397251878s
I1102 11:52:09.894848   15045 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1102 11:52:09.904260   15045 ssh_runner.go:195] Run: cat /version.json
I1102 11:52:09.904281   15045 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1102 11:52:09.904352   15045 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I1102 11:52:09.904386   15045 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1102 11:52:09.914359   15045 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/hmbl/.minikube/machines/minikube/id_rsa Username:docker}
I1102 11:52:09.914586   15045 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/hmbl/.minikube/machines/minikube/id_rsa Username:docker}
I1102 11:52:10.188110   15045 ssh_runner.go:195] Run: systemctl --version
I1102 11:52:10.193811   15045 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I1102 11:52:10.196302   15045 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I1102 11:52:10.208630   15045 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I1102 11:52:10.208659   15045 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I1102 11:52:10.213767   15045 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I1102 11:52:10.213774   15045 start.go:466] detecting cgroup driver to use...
I1102 11:52:10.213792   15045 detect.go:199] detected "systemd" cgroup driver on host os
I1102 11:52:10.213860   15045 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I1102 11:52:10.223429   15045 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I1102 11:52:10.229736   15045 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I1102 11:52:10.235543   15045 containerd.go:145] configuring containerd to use "systemd" as cgroup driver...
I1102 11:52:10.235563   15045 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = true|g' /etc/containerd/config.toml"
I1102 11:52:10.241360   15045 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1102 11:52:10.247085   15045 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I1102 11:52:10.253006   15045 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1102 11:52:10.259050   15045 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I1102 11:52:10.264433   15045 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I1102 11:52:10.270181   15045 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I1102 11:52:10.275734   15045 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I1102 11:52:10.280624   15045 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1102 11:52:10.342346   15045 ssh_runner.go:195] Run: sudo systemctl restart containerd
I1102 11:52:10.448108   15045 start.go:466] detecting cgroup driver to use...
I1102 11:52:10.448137   15045 detect.go:199] detected "systemd" cgroup driver on host os
I1102 11:52:10.448173   15045 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I1102 11:52:10.457068   15045 cruntime.go:276] skipping containerd shutdown because we are bound to it
I1102 11:52:10.457106   15045 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I1102 11:52:10.467597   15045 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I1102 11:52:10.483191   15045 ssh_runner.go:195] Run: which cri-dockerd
I1102 11:52:10.485470   15045 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I1102 11:52:10.491587   15045 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I1102 11:52:10.507707   15045 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I1102 11:52:10.574669   15045 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I1102 11:52:10.641216   15045 docker.go:535] configuring docker to use "systemd" as cgroup driver...
I1102 11:52:10.641232   15045 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (143 bytes)
I1102 11:52:10.653375   15045 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1102 11:52:10.718439   15045 ssh_runner.go:195] Run: sudo systemctl restart docker
I1102 11:52:12.399764   15045 ssh_runner.go:235] Completed: sudo systemctl restart docker: (1.681304855s)
I1102 11:52:12.399801   15045 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I1102 11:52:12.462837   15045 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I1102 11:52:12.535119   15045 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I1102 11:52:12.595431   15045 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1102 11:52:12.661065   15045 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I1102 11:52:12.718647   15045 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1102 11:52:12.781228   15045 ssh_runner.go:195] Run: sudo systemctl restart cri-docker
I1102 11:52:13.014189   15045 start.go:513] Will wait 60s for socket path /var/run/cri-dockerd.sock
I1102 11:52:13.014253   15045 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I1102 11:52:13.016592   15045 start.go:534] Will wait 60s for crictl version
I1102 11:52:13.016624   15045 ssh_runner.go:195] Run: which crictl
I1102 11:52:13.019055   15045 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I1102 11:52:13.144607   15045 start.go:550] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  24.0.4
RuntimeApiVersion:  v1
I1102 11:52:13.144637   15045 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1102 11:52:13.230462   15045 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1102 11:52:13.246018   15045 out.go:204] üê≥  Preparing Kubernetes v1.27.4 on Docker 24.0.4 ...
I1102 11:52:13.246396   15045 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I1102 11:52:13.256302   15045 ssh_runner.go:195] Run: grep 192.168.49.1	host.minikube.internal$ /etc/hosts
I1102 11:52:13.258904   15045 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.49.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1102 11:52:13.266185   15045 preload.go:132] Checking if preload exists for k8s version v1.27.4 and runtime docker
I1102 11:52:13.266214   15045 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1102 11:52:13.276686   15045 docker.go:636] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.27.4
registry.k8s.io/kube-scheduler:v1.27.4
registry.k8s.io/kube-controller-manager:v1.27.4
registry.k8s.io/kube-proxy:v1.27.4
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/etcd:3.5.7-0
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1102 11:52:13.276697   15045 docker.go:566] Images already preloaded, skipping extraction
I1102 11:52:13.276728   15045 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1102 11:52:13.288015   15045 docker.go:636] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.27.4
registry.k8s.io/kube-scheduler:v1.27.4
registry.k8s.io/kube-controller-manager:v1.27.4
registry.k8s.io/kube-proxy:v1.27.4
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/etcd:3.5.7-0
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1102 11:52:13.288021   15045 cache_images.go:84] Images are preloaded, skipping loading
I1102 11:52:13.288043   15045 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I1102 11:52:13.462096   15045 cni.go:84] Creating CNI manager for ""
I1102 11:52:13.462104   15045 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1102 11:52:13.462110   15045 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I1102 11:52:13.462121   15045 kubeadm.go:176] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.27.4 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:systemd ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I1102 11:52:13.462197   15045 kubeadm.go:181] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.27.4
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: systemd
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I1102 11:52:13.462232   15045 kubeadm.go:976] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.27.4/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:///var/run/cri-dockerd.sock --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.27.4 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I1102 11:52:13.462261   15045 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.27.4
I1102 11:52:13.468548   15045 binaries.go:44] Found k8s binaries, skipping transfer
I1102 11:52:13.468573   15045 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I1102 11:52:13.473626   15045 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (369 bytes)
I1102 11:52:13.484538   15045 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I1102 11:52:13.495092   15045 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2090 bytes)
I1102 11:52:13.505827   15045 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I1102 11:52:13.507570   15045 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1102 11:52:13.513839   15045 certs.go:56] Setting up /home/hmbl/.minikube/profiles/minikube for IP: 192.168.49.2
I1102 11:52:13.513851   15045 certs.go:190] acquiring lock for shared ca certs: {Name:mk50994dc1ea3a2280eca6f8e5a141e6ae6098cd Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1102 11:52:13.513961   15045 certs.go:199] skipping minikubeCA CA generation: /home/hmbl/.minikube/ca.key
I1102 11:52:13.514143   15045 certs.go:199] skipping proxyClientCA CA generation: /home/hmbl/.minikube/proxy-client-ca.key
I1102 11:52:13.514183   15045 certs.go:315] skipping minikube-user signed cert generation: /home/hmbl/.minikube/profiles/minikube/client.key
I1102 11:52:13.514357   15045 certs.go:315] skipping minikube signed cert generation: /home/hmbl/.minikube/profiles/minikube/apiserver.key.dd3b5fb2
I1102 11:52:13.514523   15045 certs.go:315] skipping aggregator signed cert generation: /home/hmbl/.minikube/profiles/minikube/proxy-client.key
I1102 11:52:13.514588   15045 certs.go:437] found cert: /home/hmbl/.minikube/certs/home/hmbl/.minikube/certs/ca-key.pem (1675 bytes)
I1102 11:52:13.514605   15045 certs.go:437] found cert: /home/hmbl/.minikube/certs/home/hmbl/.minikube/certs/ca.pem (1074 bytes)
I1102 11:52:13.514618   15045 certs.go:437] found cert: /home/hmbl/.minikube/certs/home/hmbl/.minikube/certs/cert.pem (1115 bytes)
I1102 11:52:13.514631   15045 certs.go:437] found cert: /home/hmbl/.minikube/certs/home/hmbl/.minikube/certs/key.pem (1679 bytes)
I1102 11:52:13.514998   15045 ssh_runner.go:362] scp /home/hmbl/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1399 bytes)
I1102 11:52:13.529839   15045 ssh_runner.go:362] scp /home/hmbl/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I1102 11:52:13.544274   15045 ssh_runner.go:362] scp /home/hmbl/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I1102 11:52:13.558555   15045 ssh_runner.go:362] scp /home/hmbl/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I1102 11:52:13.572185   15045 ssh_runner.go:362] scp /home/hmbl/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I1102 11:52:13.587111   15045 ssh_runner.go:362] scp /home/hmbl/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I1102 11:52:13.601846   15045 ssh_runner.go:362] scp /home/hmbl/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I1102 11:52:13.616417   15045 ssh_runner.go:362] scp /home/hmbl/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I1102 11:52:13.630900   15045 ssh_runner.go:362] scp /home/hmbl/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I1102 11:52:13.645676   15045 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I1102 11:52:13.657056   15045 ssh_runner.go:195] Run: openssl version
I1102 11:52:13.663366   15045 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I1102 11:52:13.670037   15045 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I1102 11:52:13.671904   15045 certs.go:480] hashing: -rw-r--r-- 1 root root 1111 Aug 21 06:31 /usr/share/ca-certificates/minikubeCA.pem
I1102 11:52:13.671921   15045 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I1102 11:52:13.675767   15045 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I1102 11:52:13.681067   15045 ssh_runner.go:195] Run: ls /var/lib/minikube/certs/etcd
I1102 11:52:13.682997   15045 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I1102 11:52:13.686802   15045 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I1102 11:52:13.690448   15045 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I1102 11:52:13.694021   15045 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I1102 11:52:13.697687   15045 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I1102 11:52:13.701342   15045 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I1102 11:52:13.704939   15045 kubeadm.go:404] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 Memory:3900 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.27.4 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.27.4 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/hmbl:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0}
I1102 11:52:13.705009   15045 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I1102 11:52:13.715161   15045 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I1102 11:52:13.721039   15045 kubeadm.go:419] found existing configuration files, will attempt cluster restart
I1102 11:52:13.721042   15045 kubeadm.go:636] restartCluster start
I1102 11:52:13.721056   15045 ssh_runner.go:195] Run: sudo test -d /data/minikube
I1102 11:52:13.726012   15045 kubeadm.go:127] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I1102 11:52:13.726344   15045 kubeconfig.go:92] found "minikube" server: "https://192.168.49.2:8443"
I1102 11:52:13.732966   15045 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I1102 11:52:13.737995   15045 api_server.go:166] Checking apiserver status ...
I1102 11:52:13.738014   15045 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1102 11:52:13.744619   15045 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1102 11:52:13.744622   15045 api_server.go:166] Checking apiserver status ...
I1102 11:52:13.744637   15045 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1102 11:52:13.750274   15045 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1102 11:52:14.250409   15045 api_server.go:166] Checking apiserver status ...
I1102 11:52:14.250436   15045 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1102 11:52:14.257002   15045 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1102 11:52:14.751202   15045 api_server.go:166] Checking apiserver status ...
I1102 11:52:14.751257   15045 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1102 11:52:14.759048   15045 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1102 11:52:15.251092   15045 api_server.go:166] Checking apiserver status ...
I1102 11:52:15.251125   15045 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1102 11:52:15.257568   15045 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1102 11:52:15.750857   15045 api_server.go:166] Checking apiserver status ...
I1102 11:52:15.750891   15045 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1102 11:52:15.757594   15045 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1102 11:52:16.250753   15045 api_server.go:166] Checking apiserver status ...
I1102 11:52:16.250795   15045 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1102 11:52:16.257489   15045 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1102 11:52:16.750772   15045 api_server.go:166] Checking apiserver status ...
I1102 11:52:16.750803   15045 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1102 11:52:16.757460   15045 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1102 11:52:17.250766   15045 api_server.go:166] Checking apiserver status ...
I1102 11:52:17.250809   15045 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1102 11:52:17.258373   15045 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1102 11:52:17.750758   15045 api_server.go:166] Checking apiserver status ...
I1102 11:52:17.750809   15045 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1102 11:52:17.757773   15045 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1102 11:52:18.251060   15045 api_server.go:166] Checking apiserver status ...
I1102 11:52:18.251094   15045 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1102 11:52:18.257792   15045 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1102 11:52:18.750981   15045 api_server.go:166] Checking apiserver status ...
I1102 11:52:18.751018   15045 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1102 11:52:18.757645   15045 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1102 11:52:19.250985   15045 api_server.go:166] Checking apiserver status ...
I1102 11:52:19.251016   15045 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1102 11:52:19.257241   15045 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1102 11:52:19.750503   15045 api_server.go:166] Checking apiserver status ...
I1102 11:52:19.750534   15045 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1102 11:52:19.757215   15045 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1102 11:52:20.250542   15045 api_server.go:166] Checking apiserver status ...
I1102 11:52:20.250578   15045 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1102 11:52:20.257275   15045 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1102 11:52:20.750625   15045 api_server.go:166] Checking apiserver status ...
I1102 11:52:20.750652   15045 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1102 11:52:20.757288   15045 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1102 11:52:21.250613   15045 api_server.go:166] Checking apiserver status ...
I1102 11:52:21.250649   15045 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1102 11:52:21.257312   15045 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1102 11:52:21.750609   15045 api_server.go:166] Checking apiserver status ...
I1102 11:52:21.750648   15045 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1102 11:52:21.757502   15045 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1102 11:52:22.250777   15045 api_server.go:166] Checking apiserver status ...
I1102 11:52:22.250815   15045 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1102 11:52:22.257492   15045 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1102 11:52:22.750780   15045 api_server.go:166] Checking apiserver status ...
I1102 11:52:22.750814   15045 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1102 11:52:22.757520   15045 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1102 11:52:23.250752   15045 api_server.go:166] Checking apiserver status ...
I1102 11:52:23.250784   15045 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1102 11:52:23.257562   15045 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1102 11:52:23.738959   15045 kubeadm.go:611] needs reconfigure: apiserver error: context deadline exceeded
I1102 11:52:23.738973   15045 kubeadm.go:1128] stopping kube-system containers ...
I1102 11:52:23.739003   15045 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I1102 11:52:23.751324   15045 docker.go:462] Stopping containers: [fd53a55e3bb5 5b615090f30c 19fc074a58f7 c69bfbeff9a9 37ae95347aee 13df457c38ad 86248806f5b6 0db747a2142d 7d8b731efd93 a4edd846b3ed 9de7082db4f8 6a2947186a3f 0efb9afb4548 d08964dfb104 bcddef593494 39d42c6af9e3 9e998d0b01a1 442adff24a3a 819e7d75b1d3 aac2549d744a 461c96a71689 79c278e603a5 fb471d96656d 7ee1f960d420 9278757b5ac0 63cbf9a35e24 f5d594d400f2]
I1102 11:52:23.751355   15045 ssh_runner.go:195] Run: docker stop fd53a55e3bb5 5b615090f30c 19fc074a58f7 c69bfbeff9a9 37ae95347aee 13df457c38ad 86248806f5b6 0db747a2142d 7d8b731efd93 a4edd846b3ed 9de7082db4f8 6a2947186a3f 0efb9afb4548 d08964dfb104 bcddef593494 39d42c6af9e3 9e998d0b01a1 442adff24a3a 819e7d75b1d3 aac2549d744a 461c96a71689 79c278e603a5 fb471d96656d 7ee1f960d420 9278757b5ac0 63cbf9a35e24 f5d594d400f2
I1102 11:52:23.762571   15045 ssh_runner.go:195] Run: sudo systemctl stop kubelet
I1102 11:52:23.770366   15045 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I1102 11:52:23.775438   15045 kubeadm.go:155] found existing configuration files:
-rw------- 1 root root 5643 Aug 21 06:31 /etc/kubernetes/admin.conf
-rw------- 1 root root 5652 Oct  5 13:07 /etc/kubernetes/controller-manager.conf
-rw------- 1 root root 1971 Aug 21 06:31 /etc/kubernetes/kubelet.conf
-rw------- 1 root root 5600 Oct  5 13:07 /etc/kubernetes/scheduler.conf

I1102 11:52:23.775456   15045 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I1102 11:52:23.780606   15045 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I1102 11:52:23.785741   15045 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I1102 11:52:23.791223   15045 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 1
stdout:

stderr:
I1102 11:52:23.791248   15045 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I1102 11:52:23.796965   15045 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I1102 11:52:23.802040   15045 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 1
stdout:

stderr:
I1102 11:52:23.802056   15045 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I1102 11:52:23.807025   15045 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I1102 11:52:23.812119   15045 kubeadm.go:713] reconfiguring cluster from /var/tmp/minikube/kubeadm.yaml
I1102 11:52:23.812123   15045 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.4:$PATH" kubeadm init phase certs all --config /var/tmp/minikube/kubeadm.yaml"
I1102 11:52:23.975874   15045 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.4:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml"
I1102 11:52:24.357606   15045 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.4:$PATH" kubeadm init phase kubelet-start --config /var/tmp/minikube/kubeadm.yaml"
I1102 11:52:24.483699   15045 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.4:$PATH" kubeadm init phase control-plane all --config /var/tmp/minikube/kubeadm.yaml"
I1102 11:52:24.514387   15045 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.4:$PATH" kubeadm init phase etcd local --config /var/tmp/minikube/kubeadm.yaml"
I1102 11:52:24.544376   15045 api_server.go:52] waiting for apiserver process to appear ...
I1102 11:52:24.544408   15045 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1102 11:52:24.551001   15045 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1102 11:52:25.057649   15045 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1102 11:52:25.557746   15045 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1102 11:52:26.057991   15045 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1102 11:52:26.064949   15045 api_server.go:72] duration metric: took 1.520574249s to wait for apiserver process to appear ...
I1102 11:52:26.064955   15045 api_server.go:88] waiting for apiserver healthz status ...
I1102 11:52:26.064967   15045 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1102 11:52:26.065128   15045 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1102 11:52:26.065140   15045 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1102 11:52:26.065248   15045 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1102 11:52:26.565602   15045 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1102 11:52:27.487906   15045 api_server.go:279] https://192.168.49.2:8443/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\": RBAC: clusterrole.rbac.authorization.k8s.io \"system:public-info-viewer\" not found","reason":"Forbidden","details":{},"code":403}
W1102 11:52:27.487924   15045 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\": RBAC: clusterrole.rbac.authorization.k8s.io \"system:public-info-viewer\" not found","reason":"Forbidden","details":{},"code":403}
I1102 11:52:27.487934   15045 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1102 11:52:27.492758   15045 api_server.go:279] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[-]etcd failed: reason withheld
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[-]poststarthook/start-apiextensions-controllers failed: reason withheld
[-]poststarthook/crd-informer-synced failed: reason withheld
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[-]poststarthook/priority-and-fairness-config-producer failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W1102 11:52:27.492769   15045 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[-]etcd failed: reason withheld
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[-]poststarthook/start-apiextensions-controllers failed: reason withheld
[-]poststarthook/crd-informer-synced failed: reason withheld
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[-]poststarthook/priority-and-fairness-config-producer failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I1102 11:52:27.566034   15045 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1102 11:52:27.569271   15045 api_server.go:279] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[-]etcd failed: reason withheld
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W1102 11:52:27.569278   15045 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[-]etcd failed: reason withheld
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I1102 11:52:28.065555   15045 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1102 11:52:28.068873   15045 api_server.go:279] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W1102 11:52:28.068881   15045 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I1102 11:52:28.566184   15045 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1102 11:52:28.569384   15045 api_server.go:279] https://192.168.49.2:8443/healthz returned 200:
ok
I1102 11:52:28.576721   15045 api_server.go:141] control plane version: v1.27.4
I1102 11:52:28.576727   15045 api_server.go:131] duration metric: took 2.511769111s to wait for apiserver health ...
I1102 11:52:28.576731   15045 cni.go:84] Creating CNI manager for ""
I1102 11:52:28.576738   15045 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1102 11:52:28.578454   15045 out.go:177] üîó  Configuring bridge CNI (Container Networking Interface) ...
I1102 11:52:28.580119   15045 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I1102 11:52:28.586146   15045 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (457 bytes)
I1102 11:52:28.598166   15045 system_pods.go:43] waiting for kube-system pods to appear ...
I1102 11:52:28.604596   15045 system_pods.go:59] 7 kube-system pods found
I1102 11:52:28.604606   15045 system_pods.go:61] "coredns-5d78c9869d-vn2zm" [79df00e0-e545-4661-8718-a8a369e9199d] Running
I1102 11:52:28.604612   15045 system_pods.go:61] "etcd-minikube" [bda46f26-a9f2-4df6-bf20-040f8f2e483d] Running
I1102 11:52:28.604615   15045 system_pods.go:61] "kube-apiserver-minikube" [21c60612-f7d5-41c0-9320-154cf2de4dc7] Running
I1102 11:52:28.604618   15045 system_pods.go:61] "kube-controller-manager-minikube" [7cb33dfa-3956-47d1-8344-d8a611002636] Running
I1102 11:52:28.604620   15045 system_pods.go:61] "kube-proxy-9qftq" [2abeaddf-a792-44f4-997c-9e50b8f945a2] Running
I1102 11:52:28.604622   15045 system_pods.go:61] "kube-scheduler-minikube" [27229f75-8411-4ffd-8812-4076bfd19bdc] Running
I1102 11:52:28.604624   15045 system_pods.go:61] "storage-provisioner" [b59ed751-35e4-41f4-840e-39c491696f78] Running
I1102 11:52:28.604626   15045 system_pods.go:74] duration metric: took 6.454465ms to wait for pod list to return data ...
I1102 11:52:28.604629   15045 node_conditions.go:102] verifying NodePressure condition ...
I1102 11:52:28.606163   15045 node_conditions.go:122] node storage ephemeral capacity is 261789844Ki
I1102 11:52:28.606169   15045 node_conditions.go:123] node cpu capacity is 16
I1102 11:52:28.606175   15045 node_conditions.go:105] duration metric: took 1.543664ms to run NodePressure ...
I1102 11:52:28.606182   15045 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.4:$PATH" kubeadm init phase addon all --config /var/tmp/minikube/kubeadm.yaml"
I1102 11:52:28.720835   15045 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I1102 11:52:28.725347   15045 ops.go:34] apiserver oom_adj: -16
I1102 11:52:28.725352   15045 kubeadm.go:640] restartCluster took 15.004307248s
I1102 11:52:28.725356   15045 kubeadm.go:406] StartCluster complete in 15.020420262s
I1102 11:52:28.725367   15045 settings.go:142] acquiring lock: {Name:mk9930a3045b502fb6dea3d7b1de7041c33e6902 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1102 11:52:28.725431   15045 settings.go:150] Updating kubeconfig:  /home/hmbl/.kube/config
I1102 11:52:28.725782   15045 lock.go:35] WriteFile acquiring /home/hmbl/.kube/config: {Name:mk9f546af09c009d961d80ff767f6ddfe8a328e0 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1102 11:52:28.725901   15045 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.27.4/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I1102 11:52:28.726013   15045 addons.go:499] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false volumesnapshots:false]
I1102 11:52:28.726056   15045 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.27.4
I1102 11:52:28.726056   15045 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I1102 11:52:28.726058   15045 addons.go:69] Setting default-storageclass=true in profile "minikube"
I1102 11:52:28.726065   15045 addons.go:231] Setting addon storage-provisioner=true in "minikube"
I1102 11:52:28.726067   15045 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
W1102 11:52:28.726067   15045 addons.go:240] addon storage-provisioner should already be in state true
I1102 11:52:28.726093   15045 host.go:66] Checking if "minikube" exists ...
I1102 11:52:28.726254   15045 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1102 11:52:28.726302   15045 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1102 11:52:28.728169   15045 kapi.go:248] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I1102 11:52:28.728180   15045 start.go:223] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.27.4 ContainerRuntime:docker ControlPlane:true Worker:true}
I1102 11:52:28.730173   15045 out.go:177] üîé  Verifying Kubernetes components...
I1102 11:52:28.733647   15045 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I1102 11:52:28.738647   15045 out.go:177]     ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I1102 11:52:28.741722   15045 addons.go:423] installing /etc/kubernetes/addons/storage-provisioner.yaml
I1102 11:52:28.741728   15045 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I1102 11:52:28.740315   15045 addons.go:231] Setting addon default-storageclass=true in "minikube"
W1102 11:52:28.741733   15045 addons.go:240] addon default-storageclass should already be in state true
I1102 11:52:28.741748   15045 host.go:66] Checking if "minikube" exists ...
I1102 11:52:28.741754   15045 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1102 11:52:28.742121   15045 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1102 11:52:28.753514   15045 addons.go:423] installing /etc/kubernetes/addons/storageclass.yaml
I1102 11:52:28.753523   15045 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I1102 11:52:28.753558   15045 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1102 11:52:28.753770   15045 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/hmbl/.minikube/machines/minikube/id_rsa Username:docker}
I1102 11:52:28.764757   15045 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/hmbl/.minikube/machines/minikube/id_rsa Username:docker}
I1102 11:52:28.842538   15045 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.27.4/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I1102 11:52:28.853340   15045 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.27.4/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I1102 11:52:28.937642   15045 start.go:874] CoreDNS already contains "host.minikube.internal" host record, skipping...
I1102 11:52:28.937673   15045 api_server.go:52] waiting for apiserver process to appear ...
I1102 11:52:28.937710   15045 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1102 11:52:29.388286   15045 api_server.go:72] duration metric: took 660.0872ms to wait for apiserver process to appear ...
I1102 11:52:29.388299   15045 api_server.go:88] waiting for apiserver healthz status ...
I1102 11:52:29.388310   15045 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1102 11:52:29.391529   15045 out.go:177] üåü  Enabled addons: storage-provisioner, default-storageclass
I1102 11:52:29.394033   15045 addons.go:502] enable addons completed in 668.014483ms: enabled=[storage-provisioner default-storageclass]
I1102 11:52:29.393232   15045 api_server.go:279] https://192.168.49.2:8443/healthz returned 200:
ok
I1102 11:52:29.395054   15045 api_server.go:141] control plane version: v1.27.4
I1102 11:52:29.395061   15045 api_server.go:131] duration metric: took 6.757973ms to wait for apiserver health ...
I1102 11:52:29.395068   15045 system_pods.go:43] waiting for kube-system pods to appear ...
I1102 11:52:29.399283   15045 system_pods.go:59] 7 kube-system pods found
I1102 11:52:29.399305   15045 system_pods.go:61] "coredns-5d78c9869d-vn2zm" [79df00e0-e545-4661-8718-a8a369e9199d] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I1102 11:52:29.399312   15045 system_pods.go:61] "etcd-minikube" [bda46f26-a9f2-4df6-bf20-040f8f2e483d] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I1102 11:52:29.399320   15045 system_pods.go:61] "kube-apiserver-minikube" [21c60612-f7d5-41c0-9320-154cf2de4dc7] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I1102 11:52:29.399325   15045 system_pods.go:61] "kube-controller-manager-minikube" [7cb33dfa-3956-47d1-8344-d8a611002636] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I1102 11:52:29.399329   15045 system_pods.go:61] "kube-proxy-9qftq" [2abeaddf-a792-44f4-997c-9e50b8f945a2] Running / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I1102 11:52:29.399334   15045 system_pods.go:61] "kube-scheduler-minikube" [27229f75-8411-4ffd-8812-4076bfd19bdc] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I1102 11:52:29.399339   15045 system_pods.go:61] "storage-provisioner" [b59ed751-35e4-41f4-840e-39c491696f78] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I1102 11:52:29.399344   15045 system_pods.go:74] duration metric: took 4.272579ms to wait for pod list to return data ...
I1102 11:52:29.399354   15045 kubeadm.go:581] duration metric: took 671.161489ms to wait for : map[apiserver:true system_pods:true] ...
I1102 11:52:29.399368   15045 node_conditions.go:102] verifying NodePressure condition ...
I1102 11:52:29.401771   15045 node_conditions.go:122] node storage ephemeral capacity is 261789844Ki
I1102 11:52:29.401781   15045 node_conditions.go:123] node cpu capacity is 16
I1102 11:52:29.401791   15045 node_conditions.go:105] duration metric: took 2.419968ms to run NodePressure ...
I1102 11:52:29.401801   15045 start.go:228] waiting for startup goroutines ...
I1102 11:52:29.401805   15045 start.go:233] waiting for cluster config update ...
I1102 11:52:29.401814   15045 start.go:242] writing updated cluster config ...
I1102 11:52:29.402447   15045 ssh_runner.go:195] Run: rm -f paused
I1102 11:52:29.437307   15045 start.go:600] kubectl: 1.28.0, cluster: 1.27.4 (minor skew: 1)
I1102 11:52:29.443087   15045 out.go:177] üèÑ  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default

* 
* ==> Docker <==
* Nov 02 11:08:33 minikube dockerd[850]: time="2023-11-02T11:08:33.322599648Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Nov 02 11:08:33 minikube dockerd[850]: time="2023-11-02T11:08:33.322628308Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Nov 02 11:08:37 minikube dockerd[850]: time="2023-11-02T11:08:37.285287348Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Nov 02 11:08:37 minikube dockerd[850]: time="2023-11-02T11:08:37.285306088Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Nov 02 11:09:02 minikube dockerd[850]: time="2023-11-02T11:09:02.312514462Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Nov 02 11:09:02 minikube dockerd[850]: time="2023-11-02T11:09:02.312533542Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Nov 02 11:09:03 minikube dockerd[850]: time="2023-11-02T11:09:03.775226853Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Nov 02 11:09:03 minikube dockerd[850]: time="2023-11-02T11:09:03.775246463Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Nov 02 11:09:16 minikube dockerd[850]: time="2023-11-02T11:09:16.304468725Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Nov 02 11:09:16 minikube dockerd[850]: time="2023-11-02T11:09:16.304489396Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Nov 02 11:09:17 minikube dockerd[850]: time="2023-11-02T11:09:17.830688935Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Nov 02 11:09:17 minikube dockerd[850]: time="2023-11-02T11:09:17.830707106Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Nov 02 11:13:41 minikube dockerd[850]: time="2023-11-02T11:13:41.345828131Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Nov 02 11:13:41 minikube dockerd[850]: time="2023-11-02T11:13:41.345847831Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Nov 02 11:13:44 minikube dockerd[850]: time="2023-11-02T11:13:44.275179199Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Nov 02 11:13:44 minikube dockerd[850]: time="2023-11-02T11:13:44.275200450Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Nov 02 11:14:06 minikube dockerd[850]: time="2023-11-02T11:14:06.558369475Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Nov 02 11:14:06 minikube dockerd[850]: time="2023-11-02T11:14:06.558387636Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Nov 02 11:14:12 minikube dockerd[850]: time="2023-11-02T11:14:12.356453070Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Nov 02 11:14:12 minikube dockerd[850]: time="2023-11-02T11:14:12.356480350Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Nov 02 11:14:22 minikube dockerd[850]: time="2023-11-02T11:14:22.324650549Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Nov 02 11:14:22 minikube dockerd[850]: time="2023-11-02T11:14:22.324674229Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Nov 02 11:14:29 minikube dockerd[850]: time="2023-11-02T11:14:29.282589781Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Nov 02 11:14:29 minikube dockerd[850]: time="2023-11-02T11:14:29.282609081Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Nov 02 11:18:48 minikube dockerd[850]: time="2023-11-02T11:18:48.303767891Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Nov 02 11:18:48 minikube dockerd[850]: time="2023-11-02T11:18:48.303795511Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Nov 02 11:18:50 minikube dockerd[850]: time="2023-11-02T11:18:50.287231195Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Nov 02 11:18:50 minikube dockerd[850]: time="2023-11-02T11:18:50.287250555Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Nov 02 11:19:15 minikube dockerd[850]: time="2023-11-02T11:19:15.290953016Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Nov 02 11:19:15 minikube dockerd[850]: time="2023-11-02T11:19:15.290973427Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Nov 02 11:19:22 minikube dockerd[850]: time="2023-11-02T11:19:22.305853554Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Nov 02 11:19:22 minikube dockerd[850]: time="2023-11-02T11:19:22.305874684Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Nov 02 11:19:32 minikube dockerd[850]: time="2023-11-02T11:19:32.296239691Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Nov 02 11:19:32 minikube dockerd[850]: time="2023-11-02T11:19:32.296261821Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Nov 02 11:19:33 minikube dockerd[850]: time="2023-11-02T11:19:33.754477362Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Nov 02 11:19:33 minikube dockerd[850]: time="2023-11-02T11:19:33.754497473Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Nov 02 11:23:57 minikube dockerd[850]: time="2023-11-02T11:23:57.273937510Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Nov 02 11:23:57 minikube dockerd[850]: time="2023-11-02T11:23:57.273956999Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Nov 02 11:23:59 minikube dockerd[850]: time="2023-11-02T11:23:59.323673546Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Nov 02 11:23:59 minikube dockerd[850]: time="2023-11-02T11:23:59.323694676Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Nov 02 11:24:25 minikube dockerd[850]: time="2023-11-02T11:24:25.341138141Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Nov 02 11:24:25 minikube dockerd[850]: time="2023-11-02T11:24:25.341157300Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Nov 02 11:24:34 minikube dockerd[850]: time="2023-11-02T11:24:34.289441626Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Nov 02 11:24:34 minikube dockerd[850]: time="2023-11-02T11:24:34.289464925Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Nov 02 11:24:37 minikube dockerd[850]: time="2023-11-02T11:24:37.272355464Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Nov 02 11:24:37 minikube dockerd[850]: time="2023-11-02T11:24:37.272379714Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Nov 02 11:24:43 minikube dockerd[850]: time="2023-11-02T11:24:43.276426480Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Nov 02 11:24:43 minikube dockerd[850]: time="2023-11-02T11:24:43.276453749Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Nov 02 11:29:04 minikube dockerd[850]: time="2023-11-02T11:29:04.321874396Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Nov 02 11:29:04 minikube dockerd[850]: time="2023-11-02T11:29:04.321897536Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Nov 02 11:29:11 minikube dockerd[850]: time="2023-11-02T11:29:11.304492259Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Nov 02 11:29:11 minikube dockerd[850]: time="2023-11-02T11:29:11.304515499Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Nov 02 11:29:34 minikube dockerd[850]: time="2023-11-02T11:29:34.302568545Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Nov 02 11:29:34 minikube dockerd[850]: time="2023-11-02T11:29:34.302591924Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Nov 02 11:29:47 minikube dockerd[850]: time="2023-11-02T11:29:47.255098452Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Nov 02 11:29:47 minikube dockerd[850]: time="2023-11-02T11:29:47.255119871Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Nov 02 11:29:50 minikube dockerd[850]: time="2023-11-02T11:29:50.277066621Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Nov 02 11:29:50 minikube dockerd[850]: time="2023-11-02T11:29:50.277087151Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Nov 02 11:29:51 minikube dockerd[850]: time="2023-11-02T11:29:51.769295289Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Nov 02 11:29:51 minikube dockerd[850]: time="2023-11-02T11:29:51.769315919Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"

* 
* ==> container status <==
* CONTAINER           IMAGE               CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
c2c1a278071f2       6e38f40d628db       36 minutes ago      Running             storage-provisioner       5                   2adf2f5af6df3       storage-provisioner
6ca27e64ac8d3       ead0a4a53df89       37 minutes ago      Running             coredns                   2                   d273c10ddb224       coredns-5d78c9869d-vn2zm
fe8a4b460f3ea       6848d7eda0341       37 minutes ago      Running             kube-proxy                2                   e8b2ae734e400       kube-proxy-9qftq
2b1fed94a9da4       6e38f40d628db       37 minutes ago      Exited              storage-provisioner       4                   2adf2f5af6df3       storage-provisioner
86772659d2834       e7972205b6614       37 minutes ago      Running             kube-apiserver            2                   70e7d27fc3d3b       kube-apiserver-minikube
254858c4e3edd       f466468864b7a       37 minutes ago      Running             kube-controller-manager   2                   e7059a75c92f1       kube-controller-manager-minikube
2335c2cb2ad73       86b6af7dd652c       37 minutes ago      Running             etcd                      2                   a3fa774476455       etcd-minikube
642050a5ed8a7       98ef2570f3cde       37 minutes ago      Running             kube-scheduler            2                   03e89c891324a       kube-scheduler-minikube
5b615090f30ce       ead0a4a53df89       3 weeks ago         Exited              coredns                   1                   37ae95347aeef       coredns-5d78c9869d-vn2zm
c69bfbeff9a96       6848d7eda0341       3 weeks ago         Exited              kube-proxy                1                   13df457c38ad6       kube-proxy-9qftq
0db747a2142d8       f466468864b7a       3 weeks ago         Exited              kube-controller-manager   1                   0efb9afb45484       kube-controller-manager-minikube
7d8b731efd937       e7972205b6614       3 weeks ago         Exited              kube-apiserver            1                   d08964dfb1049       kube-apiserver-minikube
a4edd846b3edb       86b6af7dd652c       3 weeks ago         Exited              etcd                      1                   bcddef593494f       etcd-minikube
9de7082db4f88       98ef2570f3cde       3 weeks ago         Exited              kube-scheduler            1                   6a2947186a3ff       kube-scheduler-minikube

* 
* ==> coredns [5b615090f30c] <==
* [INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 05e3eaddc414b2d71a69b2e2bc6f2681fc1f4d04bcdd3acc1a41457bb7db518208b95ddfc4c9fffedc59c25a8faf458be1af4915a4a3c0d6777cb7a346bc5d86
CoreDNS-1.10.1
linux/amd64, go1.20, 055b2c3
[INFO] 127.0.0.1:60325 - 7134 "HINFO IN 1198589050099246239.3881269140913150152. udp 57 false 512" NXDOMAIN qr,rd,ra 132 0.024768434s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[WARNING] plugin/kubernetes: Kubernetes API connection failure: Get "https://10.96.0.1:443/version": dial tcp 10.96.0.1:443: i/o timeout
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s

* 
* ==> coredns [6ca27e64ac8d] <==
* [INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 05e3eaddc414b2d71a69b2e2bc6f2681fc1f4d04bcdd3acc1a41457bb7db518208b95ddfc4c9fffedc59c25a8faf458be1af4915a4a3c0d6777cb7a346bc5d86
CoreDNS-1.10.1
linux/amd64, go1.20, 055b2c3
[INFO] 127.0.0.1:36214 - 27858 "HINFO IN 1941002284883787619.1842959591405183311. udp 57 false 512" NXDOMAIN qr,rd,ra 132 0.022061807s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[WARNING] plugin/kubernetes: Kubernetes API connection failure: Get "https://10.96.0.1:443/version": dial tcp 10.96.0.1:443: i/o timeout

* 
* ==> describe nodes <==
* Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=fd7ecd9c4599bef9f04c0986c4a0187f98a4396e
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2023_08_21T08_31_23_0700
                    minikube.k8s.io/version=v1.31.2
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Mon, 21 Aug 2023 06:31:20 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Thu, 02 Nov 2023 11:30:05 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Thu, 02 Nov 2023 11:28:09 +0000   Mon, 21 Aug 2023 06:31:19 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Thu, 02 Nov 2023 11:28:09 +0000   Mon, 21 Aug 2023 06:31:19 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Thu, 02 Nov 2023 11:28:09 +0000   Mon, 21 Aug 2023 06:31:19 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Thu, 02 Nov 2023 11:28:09 +0000   Mon, 21 Aug 2023 06:31:20 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                16
  ephemeral-storage:  261789844Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             16319232Ki
  pods:               110
Allocatable:
  cpu:                16
  ephemeral-storage:  261789844Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             16319232Ki
  pods:               110
System Info:
  Machine ID:                 f0863ff1341f4995b1d8beb21b6e506a
  System UUID:                521b3e73-7f8d-4a3a-8545-47dafe834398
  Boot ID:                    97b56dc0-8008-4ed8-8f18-816171a67a71
  Kernel Version:             6.1.0-13-amd64
  OS Image:                   Ubuntu 22.04.2 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://24.0.4
  Kubelet Version:            v1.27.4
  Kube-Proxy Version:         v1.27.4
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (13 in total)
  Namespace                   Name                                    CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                    ------------  ----------  ---------------  -------------  ---
  default                     minizinc-deployment-748c66865f-qc758    500m (3%!)(MISSING)     0 (0%!)(MISSING)      512Mi (3%!)(MISSING)       0 (0%!)(MISSING)         27d
  default                     my-backend-647678857b-mrj45             0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         36m
  default                     my-backend-6fc58fc9c6-rx7fj             0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         31m
  default                     my-frontend-5f6bccb46d-fbfts            0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         37m
  default                     my-frontend-7c9f66848f-s68vg            0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         31m
  kube-system                 coredns-5d78c9869d-vn2zm                100m (0%!)(MISSING)     0 (0%!)(MISSING)      70Mi (0%!)(MISSING)        170Mi (1%!)(MISSING)     73d
  kube-system                 etcd-minikube                           100m (0%!)(MISSING)     0 (0%!)(MISSING)      100Mi (0%!)(MISSING)       0 (0%!)(MISSING)         73d
  kube-system                 kube-apiserver-minikube                 250m (1%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         73d
  kube-system                 kube-controller-manager-minikube        200m (1%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         73d
  kube-system                 kube-proxy-9qftq                        0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         73d
  kube-system                 kube-scheduler-minikube                 100m (0%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         73d
  kube-system                 storage-provisioner                     0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         73d
  mys                         minizinc-deployment-85594f856c-pghq7    500m (3%!)(MISSING)     0 (0%!)(MISSING)      512Mi (3%!)(MISSING)       0 (0%!)(MISSING)         27d
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests     Limits
  --------           --------     ------
  cpu                1750m (10%!)(MISSING)  0 (0%!)(MISSING)
  memory             1194Mi (7%!)(MISSING)  170Mi (1%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)       0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)       0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)       0 (0%!)(MISSING)
Events:
  Type    Reason                   Age                From             Message
  ----    ------                   ----               ----             -------
  Normal  Starting                 37m                kube-proxy       
  Normal  Starting                 37m                kubelet          Starting kubelet.
  Normal  NodeAllocatableEnforced  37m                kubelet          Updated Node Allocatable limit across pods
  Normal  NodeHasSufficientMemory  37m (x8 over 37m)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    37m (x8 over 37m)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     37m (x7 over 37m)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  RegisteredNode           37m                node-controller  Node minikube event: Registered Node minikube in Controller

* 
* ==> dmesg <==
* [Nov 2 09:40]   #9 #10 #11 #12 #13 #14 #15
[  +0.059653] Expanded resource Reserved due to conflict with PCI Bus 0000:00
[  +1.810504] ata2.00: supports DRM functions and may not be fully accessible
[  +0.006303] ata2.00: supports DRM functions and may not be fully accessible
[  +3.077141] device-mapper: core: CONFIG_IMA_DISABLE_HTABLE is disabled. Duplicate IMA measurements will not be recorded in the IMA log.
[  +0.062549] systemd-journald[446]: File /var/log/journal/b08de04331af4175acfae9dac0b89cfb/system.journal corrupted or uncleanly shut down, renaming and replacing.
[  +6.996285] kauditd_printk_skb: 15 callbacks suppressed
[Nov 2 09:41] systemd-journald[446]: File /var/log/journal/b08de04331af4175acfae9dac0b89cfb/user-1000.journal corrupted or uncleanly shut down, renaming and replacing.

* 
* ==> etcd [2335c2cb2ad7] <==
* {"level":"info","ts":"2023-11-02T10:52:25.922Z","caller":"embed/etcd.go:132","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"]}
{"level":"info","ts":"2023-11-02T10:52:25.924Z","caller":"embed/etcd.go:306","msg":"starting an etcd server","etcd-version":"3.5.7","git-sha":"215b53cf3","go-version":"go1.17.13","go-os":"linux","go-arch":"amd64","max-cpu-set":16,"max-cpu-available":16,"member-initialized":true,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"max-wals":5,"max-snapshots":5,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"","initial-cluster-state":"new","initial-cluster-token":"","quota-backend-bytes":2147483648,"max-request-bytes":1572864,"max-concurrent-streams":4294967295,"pre-vote":true,"initial-corrupt-check":true,"corrupt-check-time-interval":"0s","compact-check-time-enabled":false,"compact-check-time-interval":"1m0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2023-11-02T10:52:25.943Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"17.859553ms"}
{"level":"info","ts":"2023-11-02T10:52:26.065Z","caller":"etcdserver/server.go:509","msg":"recovered v2 store from snapshot","snapshot-index":20002,"snapshot-size":"7.5 kB"}
{"level":"info","ts":"2023-11-02T10:52:26.065Z","caller":"etcdserver/server.go:522","msg":"recovered v3 backend from snapshot","backend-size-bytes":1773568,"backend-size":"1.8 MB","backend-size-in-use-bytes":1265664,"backend-size-in-use":"1.3 MB"}
{"level":"info","ts":"2023-11-02T10:52:26.101Z","caller":"etcdserver/raft.go:529","msg":"restarting local member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","commit-index":24390}
{"level":"info","ts":"2023-11-02T10:52:26.103Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"info","ts":"2023-11-02T10:52:26.103Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 3"}
{"level":"info","ts":"2023-11-02T10:52:26.103Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft aec36adc501070cc [peers: [aec36adc501070cc], term: 3, commit: 24390, applied: 20002, lastindex: 24390, lastterm: 3]"}
{"level":"info","ts":"2023-11-02T10:52:26.103Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2023-11-02T10:52:26.104Z","caller":"membership/cluster.go:278","msg":"recovered/added member from store","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","recovered-remote-peer-id":"aec36adc501070cc","recovered-remote-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2023-11-02T10:52:26.104Z","caller":"membership/cluster.go:287","msg":"set cluster version from store","cluster-version":"3.5"}
{"level":"warn","ts":"2023-11-02T10:52:26.106Z","caller":"auth/store.go:1234","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2023-11-02T10:52:26.109Z","caller":"mvcc/kvstore.go:323","msg":"restored last compact revision","meta-bucket-name":"meta","meta-bucket-name-key":"finishedCompactRev","restored-compact-revision":19131}
{"level":"info","ts":"2023-11-02T10:52:26.112Z","caller":"mvcc/kvstore.go:393","msg":"kvstore restored","current-rev":19561}
{"level":"info","ts":"2023-11-02T10:52:26.116Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2023-11-02T10:52:26.119Z","caller":"etcdserver/corrupt.go:95","msg":"starting initial corruption check","local-member-id":"aec36adc501070cc","timeout":"7s"}
{"level":"info","ts":"2023-11-02T10:52:26.119Z","caller":"etcdserver/corrupt.go:165","msg":"initial corruption checking passed; no corruption","local-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2023-11-02T10:52:26.119Z","caller":"etcdserver/server.go:845","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.7","cluster-id":"fa54960ea34d58be","cluster-version":"3.5"}
{"level":"info","ts":"2023-11-02T10:52:26.119Z","caller":"etcdserver/server.go:738","msg":"started as single-node; fast-forwarding election ticks","local-member-id":"aec36adc501070cc","forward-ticks":9,"forward-duration":"900ms","election-ticks":10,"election-timeout":"1s"}
{"level":"info","ts":"2023-11-02T10:52:26.119Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2023-11-02T10:52:26.119Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2023-11-02T10:52:26.119Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2023-11-02T10:52:26.121Z","caller":"embed/etcd.go:687","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2023-11-02T10:52:26.121Z","caller":"embed/etcd.go:586","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2023-11-02T10:52:26.121Z","caller":"embed/etcd.go:558","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2023-11-02T10:52:26.121Z","caller":"embed/etcd.go:275","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2023-11-02T10:52:26.121Z","caller":"embed/etcd.go:762","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2023-11-02T10:52:26.605Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 3"}
{"level":"info","ts":"2023-11-02T10:52:26.605Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 3"}
{"level":"info","ts":"2023-11-02T10:52:26.605Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 3"}
{"level":"info","ts":"2023-11-02T10:52:26.605Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 4"}
{"level":"info","ts":"2023-11-02T10:52:26.605Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 4"}
{"level":"info","ts":"2023-11-02T10:52:26.605Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 4"}
{"level":"info","ts":"2023-11-02T10:52:26.605Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 4"}
{"level":"info","ts":"2023-11-02T10:52:26.608Z","caller":"embed/serve.go:100","msg":"ready to serve client requests"}
{"level":"info","ts":"2023-11-02T10:52:26.608Z","caller":"etcdserver/server.go:2062","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2023-11-02T10:52:26.608Z","caller":"embed/serve.go:100","msg":"ready to serve client requests"}
{"level":"info","ts":"2023-11-02T10:52:26.608Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2023-11-02T10:52:26.608Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2023-11-02T10:52:26.609Z","caller":"embed/serve.go:198","msg":"serving client traffic securely","address":"192.168.49.2:2379"}
{"level":"info","ts":"2023-11-02T10:52:26.609Z","caller":"embed/serve.go:198","msg":"serving client traffic securely","address":"127.0.0.1:2379"}
{"level":"info","ts":"2023-11-02T11:02:26.719Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":20038}
{"level":"info","ts":"2023-11-02T11:02:26.734Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":20038,"took":"14.911482ms","hash":2500065399}
{"level":"info","ts":"2023-11-02T11:02:26.734Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2500065399,"revision":20038,"compact-revision":19131}
{"level":"info","ts":"2023-11-02T11:07:26.724Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":20395}
{"level":"info","ts":"2023-11-02T11:07:26.724Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":20395,"took":"502.204¬µs","hash":1460123254}
{"level":"info","ts":"2023-11-02T11:07:26.724Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1460123254,"revision":20395,"compact-revision":20038}
{"level":"info","ts":"2023-11-02T11:12:26.728Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":20654}
{"level":"info","ts":"2023-11-02T11:12:26.729Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":20654,"took":"393.985¬µs","hash":3489909391}
{"level":"info","ts":"2023-11-02T11:12:26.729Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3489909391,"revision":20654,"compact-revision":20395}
{"level":"info","ts":"2023-11-02T11:17:26.732Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":20914}
{"level":"info","ts":"2023-11-02T11:17:26.733Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":20914,"took":"398.246¬µs","hash":1859379854}
{"level":"info","ts":"2023-11-02T11:17:26.733Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1859379854,"revision":20914,"compact-revision":20654}
{"level":"info","ts":"2023-11-02T11:22:26.736Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":21174}
{"level":"info","ts":"2023-11-02T11:22:26.737Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":21174,"took":"388.616¬µs","hash":3404081003}
{"level":"info","ts":"2023-11-02T11:22:26.737Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3404081003,"revision":21174,"compact-revision":20914}
{"level":"info","ts":"2023-11-02T11:27:26.741Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":21437}
{"level":"info","ts":"2023-11-02T11:27:26.741Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":21437,"took":"410.832¬µs","hash":3621813871}
{"level":"info","ts":"2023-11-02T11:27:26.741Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3621813871,"revision":21437,"compact-revision":21174}

* 
* ==> etcd [a4edd846b3ed] <==
* {"level":"info","ts":"2023-10-05T13:27:52.863Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":15349,"took":"716.81¬µs","hash":2397101287}
{"level":"info","ts":"2023-10-05T13:27:52.863Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2397101287,"revision":15349,"compact-revision":15104}
{"level":"info","ts":"2023-10-05T13:32:52.866Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":15589}
{"level":"info","ts":"2023-10-05T13:32:52.866Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":15589,"took":"330.661¬µs","hash":2537797804}
{"level":"info","ts":"2023-10-05T13:32:52.867Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2537797804,"revision":15589,"compact-revision":15349}
{"level":"info","ts":"2023-10-05T13:36:53.814Z","caller":"etcdserver/server.go:1395","msg":"triggering snapshot","local-member-id":"aec36adc501070cc","local-member-applied-index":20002,"local-member-snapshot-index":10001,"local-member-snapshot-count":10000}
{"level":"info","ts":"2023-10-05T13:36:53.818Z","caller":"etcdserver/server.go:2413","msg":"saved snapshot","snapshot-index":20002}
{"level":"info","ts":"2023-10-05T13:36:53.818Z","caller":"etcdserver/server.go:2443","msg":"compacted Raft logs","compact-index":15002}
{"level":"info","ts":"2023-10-05T13:37:39.456Z","caller":"traceutil/trace.go:171","msg":"trace[1821509143] transaction","detail":"{read_only:false; response_revision:16058; number_of_response:1; }","duration":"144.212305ms","start":"2023-10-05T13:37:39.312Z","end":"2023-10-05T13:37:39.456Z","steps":["trace[1821509143] 'process raft request'  (duration: 144.135075ms)"],"step_count":1}
{"level":"info","ts":"2023-10-05T13:37:52.870Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":15828}
{"level":"info","ts":"2023-10-05T13:37:52.871Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":15828,"took":"369.901¬µs","hash":124929648}
{"level":"info","ts":"2023-10-05T13:37:52.871Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":124929648,"revision":15828,"compact-revision":15589}
{"level":"info","ts":"2023-10-05T13:38:22.226Z","caller":"traceutil/trace.go:171","msg":"trace[1135747936] transaction","detail":"{read_only:false; response_revision:16094; number_of_response:1; }","duration":"118.565345ms","start":"2023-10-05T13:38:22.107Z","end":"2023-10-05T13:38:22.226Z","steps":["trace[1135747936] 'process raft request'  (duration: 118.480755ms)"],"step_count":1}
{"level":"info","ts":"2023-10-05T13:39:50.896Z","caller":"traceutil/trace.go:171","msg":"trace[545021338] transaction","detail":"{read_only:false; response_revision:16164; number_of_response:1; }","duration":"482.923765ms","start":"2023-10-05T13:39:50.413Z","end":"2023-10-05T13:39:50.896Z","steps":["trace[545021338] 'process raft request'  (duration: 482.855615ms)"],"step_count":1}
{"level":"warn","ts":"2023-10-05T13:39:50.896Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-10-05T13:39:50.413Z","time spent":"482.982665ms","remote":"127.0.0.1:45832","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1093,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:16163 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"info","ts":"2023-10-05T13:42:52.876Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":16070}
{"level":"info","ts":"2023-10-05T13:42:52.878Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":16070,"took":"372.09¬µs","hash":2504511761}
{"level":"info","ts":"2023-10-05T13:42:52.878Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2504511761,"revision":16070,"compact-revision":15828}
{"level":"info","ts":"2023-10-05T13:47:52.881Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":16309}
{"level":"info","ts":"2023-10-05T13:47:52.881Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":16309,"took":"358.61¬µs","hash":637661315}
{"level":"info","ts":"2023-10-05T13:47:52.881Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":637661315,"revision":16309,"compact-revision":16070}
{"level":"info","ts":"2023-10-05T13:52:52.885Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":16551}
{"level":"info","ts":"2023-10-05T13:52:52.886Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":16551,"took":"367.34¬µs","hash":1634329979}
{"level":"info","ts":"2023-10-05T13:52:52.886Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1634329979,"revision":16551,"compact-revision":16309}
{"level":"info","ts":"2023-10-05T13:57:52.890Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":16790}
{"level":"info","ts":"2023-10-05T13:57:52.890Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":16790,"took":"357.202¬µs","hash":1448924846}
{"level":"info","ts":"2023-10-05T13:57:52.890Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1448924846,"revision":16790,"compact-revision":16551}
{"level":"info","ts":"2023-10-05T14:02:52.895Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":17031}
{"level":"info","ts":"2023-10-05T14:02:52.895Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":17031,"took":"351.048¬µs","hash":1397653755}
{"level":"info","ts":"2023-10-05T14:02:52.895Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1397653755,"revision":17031,"compact-revision":16790}
{"level":"info","ts":"2023-10-05T14:07:52.899Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":17272}
{"level":"info","ts":"2023-10-05T14:07:52.900Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":17272,"took":"344.745¬µs","hash":2140934147}
{"level":"info","ts":"2023-10-05T14:07:52.900Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2140934147,"revision":17272,"compact-revision":17031}
{"level":"info","ts":"2023-10-05T14:12:52.904Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":17511}
{"level":"info","ts":"2023-10-05T14:12:52.904Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":17511,"took":"332.454¬µs","hash":1751292425}
{"level":"info","ts":"2023-10-05T14:12:52.904Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1751292425,"revision":17511,"compact-revision":17272}
{"level":"info","ts":"2023-10-05T14:17:52.908Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":17755}
{"level":"info","ts":"2023-10-05T14:17:52.909Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":17755,"took":"376.204¬µs","hash":2566135076}
{"level":"info","ts":"2023-10-05T14:17:52.909Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2566135076,"revision":17755,"compact-revision":17511}
{"level":"info","ts":"2023-10-05T14:22:52.912Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":17995}
{"level":"info","ts":"2023-10-05T14:22:52.913Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":17995,"took":"835.317¬µs","hash":2840655253}
{"level":"info","ts":"2023-10-05T14:22:52.913Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2840655253,"revision":17995,"compact-revision":17755}
{"level":"info","ts":"2023-10-05T14:27:52.917Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":18236}
{"level":"info","ts":"2023-10-05T14:27:52.917Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":18236,"took":"323.842¬µs","hash":751735162}
{"level":"info","ts":"2023-10-05T14:27:52.917Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":751735162,"revision":18236,"compact-revision":17995}
{"level":"info","ts":"2023-10-05T14:32:52.921Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":18521}
{"level":"info","ts":"2023-10-05T14:32:52.922Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":18521,"took":"466.636¬µs","hash":2225401037}
{"level":"info","ts":"2023-10-05T14:32:52.922Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2225401037,"revision":18521,"compact-revision":18236}
{"level":"info","ts":"2023-10-05T14:37:52.928Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":18880}
{"level":"info","ts":"2023-10-05T14:37:52.929Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":18880,"took":"439.814¬µs","hash":2349954972}
{"level":"info","ts":"2023-10-05T14:37:52.929Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2349954972,"revision":18880,"compact-revision":18521}
{"level":"info","ts":"2023-10-05T14:42:52.933Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":19131}
{"level":"info","ts":"2023-10-05T14:42:52.933Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":19131,"took":"366.804¬µs","hash":2511584656}
{"level":"info","ts":"2023-10-05T14:42:52.933Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2511584656,"revision":19131,"compact-revision":18880}
{"level":"info","ts":"2023-10-05T14:45:03.172Z","caller":"osutil/interrupt_unix.go:64","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2023-10-05T14:45:03.172Z","caller":"embed/etcd.go:373","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}
{"level":"info","ts":"2023-10-05T14:45:03.184Z","caller":"etcdserver/server.go:1465","msg":"skipped leadership transfer for single voting member cluster","local-member-id":"aec36adc501070cc","current-leader-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2023-10-05T14:45:03.187Z","caller":"embed/etcd.go:568","msg":"stopping serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2023-10-05T14:45:03.189Z","caller":"embed/etcd.go:573","msg":"stopped serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2023-10-05T14:45:03.189Z","caller":"embed/etcd.go:375","msg":"closed etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}

* 
* ==> kernel <==
*  11:30:09 up  1:49,  0 users,  load average: 1.20, 0.81, 0.59
Linux minikube 6.1.0-13-amd64 #1 SMP PREEMPT_DYNAMIC Debian 6.1.55-1 (2023-09-29) x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.2 LTS"

* 
* ==> kube-apiserver [7d8b731efd93] <==
*   "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W1005 14:45:03.178201       1 logging.go:59] [core] [Channel #148 SubChannel #149] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
E1005 14:45:03.178352       1 controller.go:231] Unable to remove endpoints from kubernetes service: rpc error: code = Internal desc = server closed the stream without sending trailers
W1005 14:45:03.178359       1 logging.go:59] [core] [Channel #169 SubChannel #172] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
I1005 14:45:03.178459       1 controller.go:159] Shutting down quota evaluator
I1005 14:45:03.178468       1 controller.go:178] quota evaluator worker shutdown
I1005 14:45:03.178469       1 dynamic_cafile_content.go:171] "Shutting down controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I1005 14:45:03.178498       1 controller.go:178] quota evaluator worker shutdown
I1005 14:45:03.178505       1 controller.go:178] quota evaluator worker shutdown
I1005 14:45:03.178509       1 controller.go:178] quota evaluator worker shutdown
I1005 14:45:03.178514       1 controller.go:178] quota evaluator worker shutdown
I1005 14:45:03.178533       1 gc_controller.go:91] Shutting down apiserver lease garbage collector
I1005 14:45:03.178569       1 establishing_controller.go:87] Shutting down EstablishingController
I1005 14:45:03.178656       1 storage_flowcontrol.go:179] APF bootstrap ensurer is exiting
I1005 14:45:03.178672       1 cluster_authentication_trust_controller.go:463] Shutting down cluster_authentication_trust_controller controller
I1005 14:45:03.179178       1 object_count_tracker.go:151] "StorageObjectCountTracker pruner is exiting"
I1005 14:45:03.179192       1 apiservice_controller.go:131] Shutting down APIServiceRegistrationController
I1005 14:45:03.179495       1 controller.go:134] Ending legacy_token_tracking_controller
I1005 14:45:03.179509       1 controller.go:135] Shutting down legacy_token_tracking_controller
I1005 14:45:03.179525       1 nonstructuralschema_controller.go:204] Shutting down NonStructuralSchemaConditionController
I1005 14:45:03.179539       1 crd_finalizer.go:278] Shutting down CRDFinalizer
I1005 14:45:03.179550       1 gc_controller.go:91] Shutting down apiserver lease garbage collector
I1005 14:45:03.179558       1 autoregister_controller.go:165] Shutting down autoregister controller
I1005 14:45:03.179571       1 available_controller.go:439] Shutting down AvailableConditionController
I1005 14:45:03.179584       1 system_namespaces_controller.go:77] Shutting down system namespaces controller
I1005 14:45:03.179592       1 controller.go:122] Shutting down OpenAPI controller
I1005 14:45:03.179603       1 apf_controller.go:373] Shutting down API Priority and Fairness config worker
I1005 14:45:03.179618       1 crdregistration_controller.go:142] Shutting down crd-autoregister controller
I1005 14:45:03.179630       1 naming_controller.go:302] Shutting down NamingConditionController
I1005 14:45:03.179641       1 customresource_discovery_controller.go:325] Shutting down DiscoveryController
I1005 14:45:03.179653       1 controller.go:115] Shutting down OpenAPI V3 controller
I1005 14:45:03.179807       1 dynamic_cafile_content.go:171] "Shutting down controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I1005 14:45:03.179945       1 dynamic_cafile_content.go:171] "Shutting down controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I1005 14:45:03.179965       1 dynamic_serving_content.go:146] "Shutting down controller" name="serving-cert::/var/lib/minikube/certs/apiserver.crt::/var/lib/minikube/certs/apiserver.key"
I1005 14:45:03.179986       1 dynamic_serving_content.go:146] "Shutting down controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I1005 14:45:03.180013       1 controller.go:89] Shutting down OpenAPI AggregationController
I1005 14:45:03.180025       1 controller.go:86] Shutting down OpenAPI V3 AggregationController
I1005 14:45:03.180127       1 secure_serving.go:255] Stopped listening on [::]:8443
I1005 14:45:03.180156       1 tlsconfig.go:255] "Shutting down DynamicServingCertificateController"
I1005 14:45:03.180268       1 dynamic_cafile_content.go:171] "Shutting down controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I1005 14:45:03.178583       1 apiapproval_controller.go:198] Shutting down KubernetesAPIApprovalPolicyConformantConditionController

* 
* ==> kube-apiserver [86772659d283] <==
* W1102 10:52:27.084246       1 genericapiserver.go:752] Skipping API apiregistration.k8s.io/v1beta1 because it has no resources.
I1102 10:52:27.420529       1 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I1102 10:52:27.420525       1 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I1102 10:52:27.420823       1 dynamic_serving_content.go:132] "Starting controller" name="serving-cert::/var/lib/minikube/certs/apiserver.crt::/var/lib/minikube/certs/apiserver.key"
I1102 10:52:27.421265       1 secure_serving.go:210] Serving securely on [::]:8443
I1102 10:52:27.421303       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I1102 10:52:27.421342       1 system_namespaces_controller.go:67] Starting system namespaces controller
I1102 10:52:27.421402       1 controller.go:80] Starting OpenAPI V3 AggregationController
I1102 10:52:27.453109       1 dynamic_serving_content.go:132] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I1102 10:52:27.454064       1 apf_controller.go:361] Starting API Priority and Fairness config controller
I1102 10:52:27.454780       1 handler_discovery.go:392] Starting ResourceDiscoveryManager
I1102 10:52:27.454946       1 cluster_authentication_trust_controller.go:440] Starting cluster_authentication_trust_controller controller
I1102 10:52:27.454986       1 shared_informer.go:311] Waiting for caches to sync for cluster_authentication_trust_controller
I1102 10:52:27.455069       1 crdregistration_controller.go:111] Starting crd-autoregister controller
I1102 10:52:27.455101       1 shared_informer.go:311] Waiting for caches to sync for crd-autoregister
I1102 10:52:27.455700       1 available_controller.go:423] Starting AvailableConditionController
I1102 10:52:27.455781       1 cache.go:32] Waiting for caches to sync for AvailableConditionController controller
I1102 10:52:27.456040       1 gc_controller.go:78] Starting apiserver lease garbage collector
I1102 10:52:27.456153       1 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I1102 10:52:27.456225       1 controller.go:83] Starting OpenAPI AggregationController
I1102 10:52:27.456155       1 gc_controller.go:78] Starting apiserver lease garbage collector
I1102 10:52:27.454782       1 apiservice_controller.go:97] Starting APIServiceRegistrationController
I1102 10:52:27.456671       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I1102 10:52:27.457213       1 controller.go:121] Starting legacy_token_tracking_controller
I1102 10:52:27.457283       1 shared_informer.go:311] Waiting for caches to sync for configmaps
I1102 10:52:27.454995       1 aggregator.go:150] waiting for initial CRD sync...
I1102 10:52:27.457563       1 customresource_discovery_controller.go:289] Starting DiscoveryController
I1102 10:52:27.456050       1 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I1102 10:52:27.458187       1 controller.go:85] Starting OpenAPI controller
I1102 10:52:27.458413       1 apiapproval_controller.go:186] Starting KubernetesAPIApprovalPolicyConformantConditionController
I1102 10:52:27.458437       1 naming_controller.go:291] Starting NamingConditionController
I1102 10:52:27.458457       1 controller.go:85] Starting OpenAPI V3 controller
I1102 10:52:27.459017       1 nonstructuralschema_controller.go:192] Starting NonStructuralSchemaConditionController
I1102 10:52:27.459059       1 crd_finalizer.go:266] Starting CRDFinalizer
I1102 10:52:27.459204       1 establishing_controller.go:76] Starting EstablishingController
I1102 10:52:27.488373       1 controller.go:624] quota admission added evaluator for: leases.coordination.k8s.io
I1102 10:52:27.492316       1 shared_informer.go:318] Caches are synced for node_authorizer
I1102 10:52:27.554206       1 apf_controller.go:366] Running API Priority and Fairness config worker
I1102 10:52:27.554216       1 apf_controller.go:369] Running API Priority and Fairness periodic rebalancing process
I1102 10:52:27.555329       1 shared_informer.go:318] Caches are synced for crd-autoregister
I1102 10:52:27.555346       1 aggregator.go:152] initial CRD sync complete...
I1102 10:52:27.555352       1 autoregister_controller.go:141] Starting autoregister controller
I1102 10:52:27.555356       1 cache.go:32] Waiting for caches to sync for autoregister controller
I1102 10:52:27.555361       1 cache.go:39] Caches are synced for autoregister controller
I1102 10:52:27.555460       1 shared_informer.go:318] Caches are synced for cluster_authentication_trust_controller
I1102 10:52:27.556560       1 cache.go:39] Caches are synced for AvailableConditionController controller
I1102 10:52:27.556970       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I1102 10:52:27.557718       1 shared_informer.go:318] Caches are synced for configmaps
I1102 10:52:28.263845       1 controller.go:132] OpenAPI AggregationController: action for item k8s_internal_local_delegation_chain_0000000000: Nothing (removed from the queue).
I1102 10:52:28.455178       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I1102 10:52:28.671221       1 controller.go:624] quota admission added evaluator for: serviceaccounts
I1102 10:52:28.676740       1 controller.go:624] quota admission added evaluator for: deployments.apps
I1102 10:52:28.696409       1 controller.go:624] quota admission added evaluator for: daemonsets.apps
I1102 10:52:28.711268       1 controller.go:624] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I1102 10:52:28.716203       1 controller.go:624] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I1102 10:52:38.625855       1 controller.go:624] quota admission added evaluator for: replicasets.apps
I1102 10:52:38.709709       1 controller.go:624] quota admission added evaluator for: endpoints
I1102 10:52:38.863141       1 controller.go:624] quota admission added evaluator for: endpointslices.discovery.k8s.io
I1102 10:58:57.113094       1 alloc.go:330] "allocated clusterIPs" service="default/my-frontend" clusterIPs=map[IPv4:10.104.172.253]
I1102 10:59:05.714535       1 alloc.go:330] "allocated clusterIPs" service="default/my-backend" clusterIPs=map[IPv4:10.104.145.240]

* 
* ==> kube-controller-manager [0db747a2142d] <==
* I1005 13:08:06.155615       1 actual_state_of_world.go:547] "Failed to update statusUpdateNeeded field in actual state of world" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I1005 13:08:06.158684       1 shared_informer.go:318] Caches are synced for job
I1005 13:08:06.160787       1 shared_informer.go:318] Caches are synced for bootstrap_signer
I1005 13:08:06.161128       1 shared_informer.go:311] Waiting for caches to sync for garbage collector
I1005 13:08:06.168781       1 shared_informer.go:318] Caches are synced for ephemeral
I1005 13:08:06.173176       1 shared_informer.go:318] Caches are synced for endpoint
I1005 13:08:06.176085       1 shared_informer.go:318] Caches are synced for GC
I1005 13:08:06.179947       1 shared_informer.go:318] Caches are synced for service account
I1005 13:08:06.179957       1 shared_informer.go:318] Caches are synced for TTL
I1005 13:08:06.179983       1 shared_informer.go:318] Caches are synced for disruption
I1005 13:08:06.229934       1 shared_informer.go:318] Caches are synced for stateful set
I1005 13:08:06.238108       1 shared_informer.go:318] Caches are synced for node
I1005 13:08:06.238144       1 range_allocator.go:174] "Sending events to api server"
I1005 13:08:06.238162       1 range_allocator.go:178] "Starting range CIDR allocator"
I1005 13:08:06.238166       1 shared_informer.go:311] Waiting for caches to sync for cidrallocator
I1005 13:08:06.238170       1 shared_informer.go:318] Caches are synced for cidrallocator
I1005 13:08:06.239228       1 shared_informer.go:318] Caches are synced for PVC protection
I1005 13:08:06.241412       1 shared_informer.go:318] Caches are synced for crt configmap
I1005 13:08:06.242536       1 shared_informer.go:318] Caches are synced for namespace
I1005 13:08:06.242545       1 shared_informer.go:318] Caches are synced for endpoint_slice
I1005 13:08:06.245045       1 shared_informer.go:318] Caches are synced for daemon sets
I1005 13:08:06.246184       1 shared_informer.go:318] Caches are synced for HPA
I1005 13:08:06.246185       1 shared_informer.go:318] Caches are synced for deployment
I1005 13:08:06.246975       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-serving
I1005 13:08:06.247848       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kube-apiserver-client
I1005 13:08:06.247854       1 shared_informer.go:318] Caches are synced for ReplicaSet
I1005 13:08:06.247887       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-client
I1005 13:08:06.249011       1 shared_informer.go:318] Caches are synced for TTL after finished
I1005 13:08:06.249073       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-legacy-unknown
I1005 13:08:06.249741       1 shared_informer.go:318] Caches are synced for certificate-csrapproving
I1005 13:08:06.250884       1 shared_informer.go:318] Caches are synced for ClusterRoleAggregator
I1005 13:08:06.250958       1 shared_informer.go:318] Caches are synced for taint
I1005 13:08:06.251040       1 taint_manager.go:206] "Starting NoExecuteTaintManager"
I1005 13:08:06.251078       1 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node minikube event: Registered Node minikube in Controller"
I1005 13:08:06.251043       1 node_lifecycle_controller.go:1223] "Initializing eviction metric for zone" zone=""
I1005 13:08:06.251113       1 taint_manager.go:211] "Sending events to api server"
I1005 13:08:06.251148       1 node_lifecycle_controller.go:875] "Missing timestamp for Node. Assuming now as a timestamp" node="minikube"
I1005 13:08:06.251184       1 node_lifecycle_controller.go:1069] "Controller detected that zone is now in new state" zone="" newState=Normal
I1005 13:08:06.255267       1 shared_informer.go:318] Caches are synced for ReplicationController
I1005 13:08:06.255274       1 shared_informer.go:318] Caches are synced for endpoint_slice_mirroring
I1005 13:08:06.264469       1 shared_informer.go:318] Caches are synced for persistent volume
I1005 13:08:06.267236       1 shared_informer.go:318] Caches are synced for expand
I1005 13:08:06.337253       1 shared_informer.go:318] Caches are synced for resource quota
I1005 13:08:06.340357       1 shared_informer.go:318] Caches are synced for PV protection
I1005 13:08:06.353087       1 shared_informer.go:318] Caches are synced for resource quota
I1005 13:08:06.366308       1 shared_informer.go:318] Caches are synced for attach detach
I1005 13:08:06.369552       1 shared_informer.go:318] Caches are synced for cronjob
I1005 13:08:06.758257       1 shared_informer.go:318] Caches are synced for garbage collector
I1005 13:08:06.758270       1 garbagecollector.go:166] "All resource monitors have synced. Proceeding to collect garbage"
I1005 13:08:06.761424       1 shared_informer.go:318] Caches are synced for garbage collector
I1005 14:25:44.934684       1 event.go:307] "Event occurred" object="mys/minizinc-deployment" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set minizinc-deployment-85594f856c to 1"
I1005 14:25:44.945912       1 event.go:307] "Event occurred" object="mys/minizinc-deployment-85594f856c" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: minizinc-deployment-85594f856c-9mszh"
I1005 14:29:08.616428       1 event.go:307] "Event occurred" object="mys/minizinc-deployment" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set minizinc-deployment-85594f856c to 1"
I1005 14:29:08.622085       1 event.go:307] "Event occurred" object="mys/minizinc-deployment-85594f856c" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: minizinc-deployment-85594f856c-74brx"
I1005 14:31:11.936340       1 event.go:307] "Event occurred" object="mys/minizinc-deployment" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set minizinc-deployment-85594f856c to 1"
I1005 14:31:11.941654       1 event.go:307] "Event occurred" object="mys/minizinc-deployment-85594f856c" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: minizinc-deployment-85594f856c-b78tf"
I1005 14:43:09.476687       1 event.go:307] "Event occurred" object="default/minizinc-deployment" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set minizinc-deployment-748c66865f to 1"
I1005 14:43:09.484298       1 event.go:307] "Event occurred" object="default/minizinc-deployment-748c66865f" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: minizinc-deployment-748c66865f-qc758"
I1005 14:44:35.491106       1 event.go:307] "Event occurred" object="mys/minizinc-deployment" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set minizinc-deployment-85594f856c to 1"
I1005 14:44:35.496899       1 event.go:307] "Event occurred" object="mys/minizinc-deployment-85594f856c" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: minizinc-deployment-85594f856c-pghq7"

* 
* ==> kube-controller-manager [254858c4e3ed] <==
* I1102 10:52:38.568725       1 shared_informer.go:311] Waiting for caches to sync for TTL after finished
I1102 10:52:38.571264       1 shared_informer.go:311] Waiting for caches to sync for resource quota
I1102 10:52:38.577023       1 shared_informer.go:311] Waiting for caches to sync for garbage collector
I1102 10:52:38.602771       1 shared_informer.go:318] Caches are synced for disruption
I1102 10:52:38.616153       1 shared_informer.go:318] Caches are synced for namespace
I1102 10:52:38.617252       1 shared_informer.go:318] Caches are synced for bootstrap_signer
I1102 10:52:38.619431       1 shared_informer.go:318] Caches are synced for deployment
I1102 10:52:38.623264       1 shared_informer.go:318] Caches are synced for HPA
I1102 10:52:38.627326       1 shared_informer.go:318] Caches are synced for ephemeral
I1102 10:52:38.627333       1 event.go:307] "Event occurred" object="default/my-frontend" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set my-frontend-5f6bccb46d to 1"
I1102 10:52:38.628410       1 shared_informer.go:318] Caches are synced for job
I1102 10:52:38.629431       1 shared_informer.go:318] Caches are synced for stateful set
I1102 10:52:38.630222       1 shared_informer.go:318] Caches are synced for expand
I1102 10:52:38.631224       1 shared_informer.go:318] Caches are synced for ClusterRoleAggregator
I1102 10:52:38.646969       1 shared_informer.go:318] Caches are synced for ReplicationController
I1102 10:52:38.647029       1 shared_informer.go:318] Caches are synced for PV protection
I1102 10:52:38.648042       1 shared_informer.go:318] Caches are synced for service account
I1102 10:52:38.653407       1 shared_informer.go:318] Caches are synced for ReplicaSet
I1102 10:52:38.654463       1 shared_informer.go:318] Caches are synced for cronjob
I1102 10:52:38.660131       1 event.go:307] "Event occurred" object="default/my-frontend-5f6bccb46d" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: my-frontend-5f6bccb46d-fbfts"
I1102 10:52:38.667037       1 shared_informer.go:318] Caches are synced for crt configmap
I1102 10:52:38.667968       1 shared_informer.go:318] Caches are synced for PVC protection
I1102 10:52:38.669057       1 shared_informer.go:318] Caches are synced for TTL after finished
I1102 10:52:38.704719       1 shared_informer.go:318] Caches are synced for endpoint
I1102 10:52:38.707881       1 shared_informer.go:318] Caches are synced for endpoint_slice_mirroring
I1102 10:52:38.735049       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-serving
I1102 10:52:38.736167       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-client
I1102 10:52:38.736228       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kube-apiserver-client
I1102 10:52:38.736293       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-legacy-unknown
I1102 10:52:38.755191       1 shared_informer.go:318] Caches are synced for certificate-csrapproving
I1102 10:52:38.765832       1 shared_informer.go:318] Caches are synced for resource quota
I1102 10:52:38.771705       1 shared_informer.go:318] Caches are synced for resource quota
I1102 10:52:38.771932       1 actual_state_of_world.go:547] "Failed to update statusUpdateNeeded field in actual state of world" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I1102 10:52:38.803047       1 shared_informer.go:318] Caches are synced for TTL
I1102 10:52:38.826722       1 shared_informer.go:318] Caches are synced for taint
I1102 10:52:38.826803       1 taint_manager.go:206] "Starting NoExecuteTaintManager"
I1102 10:52:38.826782       1 node_lifecycle_controller.go:1223] "Initializing eviction metric for zone" zone=""
I1102 10:52:38.826826       1 taint_manager.go:211] "Sending events to api server"
I1102 10:52:38.826832       1 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node minikube event: Registered Node minikube in Controller"
I1102 10:52:38.826883       1 node_lifecycle_controller.go:875] "Missing timestamp for Node. Assuming now as a timestamp" node="minikube"
I1102 10:52:38.826910       1 node_lifecycle_controller.go:1069] "Controller detected that zone is now in new state" zone="" newState=Normal
I1102 10:52:38.845439       1 shared_informer.go:318] Caches are synced for node
I1102 10:52:38.845483       1 range_allocator.go:174] "Sending events to api server"
I1102 10:52:38.845522       1 range_allocator.go:178] "Starting range CIDR allocator"
I1102 10:52:38.845526       1 shared_informer.go:311] Waiting for caches to sync for cidrallocator
I1102 10:52:38.845529       1 shared_informer.go:318] Caches are synced for cidrallocator
I1102 10:52:38.850617       1 shared_informer.go:318] Caches are synced for attach detach
I1102 10:52:38.850693       1 shared_informer.go:318] Caches are synced for GC
I1102 10:52:38.851776       1 shared_informer.go:318] Caches are synced for daemon sets
I1102 10:52:38.856929       1 shared_informer.go:318] Caches are synced for persistent volume
I1102 10:52:38.858046       1 shared_informer.go:318] Caches are synced for endpoint_slice
I1102 10:52:39.177419       1 shared_informer.go:318] Caches are synced for garbage collector
I1102 10:52:39.234189       1 shared_informer.go:318] Caches are synced for garbage collector
I1102 10:52:39.234205       1 garbagecollector.go:166] "All resource monitors have synced. Proceeding to collect garbage"
I1102 10:53:12.925233       1 event.go:307] "Event occurred" object="default/my-backend" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set my-backend-647678857b to 1"
I1102 10:53:12.930198       1 event.go:307] "Event occurred" object="default/my-backend-647678857b" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: my-backend-647678857b-mrj45"
I1102 10:58:11.263960       1 event.go:307] "Event occurred" object="default/my-frontend" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set my-frontend-7c9f66848f to 1"
I1102 10:58:11.268684       1 event.go:307] "Event occurred" object="default/my-frontend-7c9f66848f" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: my-frontend-7c9f66848f-s68vg"
I1102 10:58:16.700742       1 event.go:307] "Event occurred" object="default/my-backend" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set my-backend-6fc58fc9c6 to 1"
I1102 10:58:16.705204       1 event.go:307] "Event occurred" object="default/my-backend-6fc58fc9c6" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: my-backend-6fc58fc9c6-rx7fj"

* 
* ==> kube-proxy [c69bfbeff9a9] <==
* I1005 13:07:55.549507       1 node.go:141] Successfully retrieved node IP: 192.168.49.2
I1005 13:07:55.549567       1 server_others.go:110] "Detected node IP" address="192.168.49.2"
I1005 13:07:55.549583       1 server_others.go:554] "Using iptables proxy"
I1005 13:07:55.575889       1 server_others.go:192] "Using iptables Proxier"
I1005 13:07:55.575914       1 server_others.go:199] "kube-proxy running in dual-stack mode" ipFamily=IPv4
I1005 13:07:55.575920       1 server_others.go:200] "Creating dualStackProxier for iptables"
I1005 13:07:55.575931       1 server_others.go:484] "Detect-local-mode set to ClusterCIDR, but no IPv6 cluster CIDR defined, defaulting to no-op detect-local for IPv6"
I1005 13:07:55.576648       1 proxier.go:253] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I1005 13:07:55.577439       1 server.go:658] "Version info" version="v1.27.4"
I1005 13:07:55.577449       1 server.go:660] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1005 13:07:55.578879       1 config.go:315] "Starting node config controller"
I1005 13:07:55.578944       1 config.go:97] "Starting endpoint slice config controller"
I1005 13:07:55.578949       1 config.go:188] "Starting service config controller"
I1005 13:07:55.581055       1 shared_informer.go:311] Waiting for caches to sync for service config
I1005 13:07:55.581057       1 shared_informer.go:311] Waiting for caches to sync for node config
I1005 13:07:55.581055       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I1005 13:07:55.681558       1 shared_informer.go:318] Caches are synced for service config
I1005 13:07:55.681558       1 shared_informer.go:318] Caches are synced for endpoint slice config
I1005 13:07:55.681562       1 shared_informer.go:318] Caches are synced for node config

* 
* ==> kube-proxy [fe8a4b460f3e] <==
* I1102 10:52:29.720925       1 node.go:141] Successfully retrieved node IP: 192.168.49.2
I1102 10:52:29.720970       1 server_others.go:110] "Detected node IP" address="192.168.49.2"
I1102 10:52:29.720982       1 server_others.go:554] "Using iptables proxy"
I1102 10:52:29.759503       1 server_others.go:192] "Using iptables Proxier"
I1102 10:52:29.759522       1 server_others.go:199] "kube-proxy running in dual-stack mode" ipFamily=IPv4
I1102 10:52:29.759527       1 server_others.go:200] "Creating dualStackProxier for iptables"
I1102 10:52:29.759536       1 server_others.go:484] "Detect-local-mode set to ClusterCIDR, but no IPv6 cluster CIDR defined, defaulting to no-op detect-local for IPv6"
I1102 10:52:29.761443       1 proxier.go:253] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I1102 10:52:29.762587       1 server.go:658] "Version info" version="v1.27.4"
I1102 10:52:29.762595       1 server.go:660] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1102 10:52:29.766142       1 config.go:188] "Starting service config controller"
I1102 10:52:29.766173       1 config.go:315] "Starting node config controller"
I1102 10:52:29.766196       1 config.go:97] "Starting endpoint slice config controller"
I1102 10:52:29.767892       1 shared_informer.go:311] Waiting for caches to sync for service config
I1102 10:52:29.767895       1 shared_informer.go:311] Waiting for caches to sync for node config
I1102 10:52:29.767896       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I1102 10:52:29.868754       1 shared_informer.go:318] Caches are synced for service config
I1102 10:52:29.868830       1 shared_informer.go:318] Caches are synced for endpoint slice config
I1102 10:52:29.868830       1 shared_informer.go:318] Caches are synced for node config

* 
* ==> kube-scheduler [642050a5ed8a] <==
* I1102 10:52:26.383697       1 serving.go:348] Generated self-signed cert in-memory
I1102 10:52:27.503041       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.27.4"
I1102 10:52:27.503061       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1102 10:52:27.506896       1 requestheader_controller.go:169] Starting RequestHeaderAuthRequestController
I1102 10:52:27.506934       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
I1102 10:52:27.506935       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1102 10:52:27.507376       1 secure_serving.go:210] Serving securely on 127.0.0.1:10259
I1102 10:52:27.507418       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I1102 10:52:27.507434       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I1102 10:52:27.507435       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I1102 10:52:27.507718       1 shared_informer.go:311] Waiting for caches to sync for RequestHeaderAuthRequestController
I1102 10:52:27.607648       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I1102 10:52:27.607689       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I1102 10:52:27.607808       1 shared_informer.go:318] Caches are synced for RequestHeaderAuthRequestController

* 
* ==> kube-scheduler [9de7082db4f8] <==
* I1005 13:07:52.168903       1 serving.go:348] Generated self-signed cert in-memory
W1005 13:07:53.610618       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W1005 13:07:53.610679       1 authentication.go:368] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system": RBAC: [role.rbac.authorization.k8s.io "extension-apiserver-authentication-reader" not found, role.rbac.authorization.k8s.io "system::leader-locking-kube-scheduler" not found]
W1005 13:07:53.610702       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W1005 13:07:53.610712       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I1005 13:07:53.639391       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.27.4"
I1005 13:07:53.639405       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1005 13:07:53.642868       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1005 13:07:53.643292       1 secure_serving.go:210] Serving securely on 127.0.0.1:10259
I1005 13:07:53.643330       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I1005 13:07:53.643453       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I1005 13:07:53.744041       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
E1005 14:45:03.173577       1 scheduling_queue.go:1139] "Error while retrieving next pod from scheduling queue" err="scheduling queue is closed"
I1005 14:45:03.173610       1 secure_serving.go:255] Stopped listening on 127.0.0.1:10259
E1005 14:45:03.173849       1 run.go:74] "command failed" err="finished without leader elect"

* 
* ==> kubelet <==
* Nov 02 11:28:36 minikube kubelet[1580]: E1102 11:28:36.811643    1580 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"my-frontend\" with ImagePullBackOff: \"Back-off pulling image \\\"my-frontend\\\"\"" pod="default/my-frontend-7c9f66848f-s68vg" podUID=570da166-fa49-4dbe-ab98-3cbf2bc0a96f
Nov 02 11:28:41 minikube kubelet[1580]: E1102 11:28:41.811860    1580 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"my-backend\" with ImagePullBackOff: \"Back-off pulling image \\\"my-backend\\\"\"" pod="default/my-backend-647678857b-mrj45" podUID=0e00cdb9-097b-45a4-8f91-3ecd81ef8e98
Nov 02 11:28:42 minikube kubelet[1580]: E1102 11:28:42.811629    1580 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"my-frontend\" with ImagePullBackOff: \"Back-off pulling image \\\"my-frontend\\\"\"" pod="default/my-frontend-5f6bccb46d-fbfts" podUID=3041c1f0-a64e-4d5e-bb1c-0669ce07321c
Nov 02 11:28:42 minikube kubelet[1580]: E1102 11:28:42.811633    1580 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"my-backend\" with ImagePullBackOff: \"Back-off pulling image \\\"my-backend\\\"\"" pod="default/my-backend-6fc58fc9c6-rx7fj" podUID=1495c692-fbfc-4dc2-90e2-944f36c041e8
Nov 02 11:28:43 minikube kubelet[1580]: E1102 11:28:43.811232    1580 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"minizinc-container\" with ImagePullBackOff: \"Back-off pulling image \\\"my_minizinc_image\\\"\"" pod="mys/minizinc-deployment-85594f856c-pghq7" podUID=7b335bb8-36f0-4e5a-bbe2-e855e25d95a1
Nov 02 11:28:47 minikube kubelet[1580]: E1102 11:28:47.811839    1580 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"minizinc-container\" with ImagePullBackOff: \"Back-off pulling image \\\"my_minizinc_image:latest\\\"\"" pod="default/minizinc-deployment-748c66865f-qc758" podUID=138eb270-69fe-4b31-a50f-b9cd0d8e5c08
Nov 02 11:28:49 minikube kubelet[1580]: E1102 11:28:49.811170    1580 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"my-frontend\" with ImagePullBackOff: \"Back-off pulling image \\\"my-frontend\\\"\"" pod="default/my-frontend-7c9f66848f-s68vg" podUID=570da166-fa49-4dbe-ab98-3cbf2bc0a96f
Nov 02 11:28:54 minikube kubelet[1580]: E1102 11:28:54.811702    1580 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"minizinc-container\" with ImagePullBackOff: \"Back-off pulling image \\\"my_minizinc_image\\\"\"" pod="mys/minizinc-deployment-85594f856c-pghq7" podUID=7b335bb8-36f0-4e5a-bbe2-e855e25d95a1
Nov 02 11:28:55 minikube kubelet[1580]: E1102 11:28:55.811098    1580 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"my-backend\" with ImagePullBackOff: \"Back-off pulling image \\\"my-backend\\\"\"" pod="default/my-backend-6fc58fc9c6-rx7fj" podUID=1495c692-fbfc-4dc2-90e2-944f36c041e8
Nov 02 11:28:56 minikube kubelet[1580]: E1102 11:28:56.811089    1580 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"my-backend\" with ImagePullBackOff: \"Back-off pulling image \\\"my-backend\\\"\"" pod="default/my-backend-647678857b-mrj45" podUID=0e00cdb9-097b-45a4-8f91-3ecd81ef8e98
Nov 02 11:28:57 minikube kubelet[1580]: E1102 11:28:57.811172    1580 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"my-frontend\" with ImagePullBackOff: \"Back-off pulling image \\\"my-frontend\\\"\"" pod="default/my-frontend-5f6bccb46d-fbfts" podUID=3041c1f0-a64e-4d5e-bb1c-0669ce07321c
Nov 02 11:29:02 minikube kubelet[1580]: E1102 11:29:02.811920    1580 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"my-frontend\" with ImagePullBackOff: \"Back-off pulling image \\\"my-frontend\\\"\"" pod="default/my-frontend-7c9f66848f-s68vg" podUID=570da166-fa49-4dbe-ab98-3cbf2bc0a96f
Nov 02 11:29:04 minikube kubelet[1580]: E1102 11:29:04.323968    1580 remote_image.go:167] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for my_minizinc_image, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="my_minizinc_image:latest"
Nov 02 11:29:04 minikube kubelet[1580]: E1102 11:29:04.323997    1580 kuberuntime_image.go:53] "Failed to pull image" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for my_minizinc_image, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="my_minizinc_image:latest"
Nov 02 11:29:04 minikube kubelet[1580]: E1102 11:29:04.324057    1580 kuberuntime_manager.go:1212] container &Container{Name:minizinc-container,Image:my_minizinc_image:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{536870912 0} {<nil>}  BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lhhsj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},} start failed in pod minizinc-deployment-748c66865f-qc758_default(138eb270-69fe-4b31-a50f-b9cd0d8e5c08): ErrImagePull: rpc error: code = Unknown desc = Error response from daemon: pull access denied for my_minizinc_image, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
Nov 02 11:29:04 minikube kubelet[1580]: E1102 11:29:04.324084    1580 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"minizinc-container\" with ErrImagePull: \"rpc error: code = Unknown desc = Error response from daemon: pull access denied for my_minizinc_image, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/minizinc-deployment-748c66865f-qc758" podUID=138eb270-69fe-4b31-a50f-b9cd0d8e5c08
Nov 02 11:29:07 minikube kubelet[1580]: E1102 11:29:07.811102    1580 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"my-backend\" with ImagePullBackOff: \"Back-off pulling image \\\"my-backend\\\"\"" pod="default/my-backend-6fc58fc9c6-rx7fj" podUID=1495c692-fbfc-4dc2-90e2-944f36c041e8
Nov 02 11:29:08 minikube kubelet[1580]: E1102 11:29:08.811275    1580 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"my-backend\" with ImagePullBackOff: \"Back-off pulling image \\\"my-backend\\\"\"" pod="default/my-backend-647678857b-mrj45" podUID=0e00cdb9-097b-45a4-8f91-3ecd81ef8e98
Nov 02 11:29:09 minikube kubelet[1580]: E1102 11:29:09.811952    1580 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"my-frontend\" with ImagePullBackOff: \"Back-off pulling image \\\"my-frontend\\\"\"" pod="default/my-frontend-5f6bccb46d-fbfts" podUID=3041c1f0-a64e-4d5e-bb1c-0669ce07321c
Nov 02 11:29:11 minikube kubelet[1580]: E1102 11:29:11.306572    1580 remote_image.go:167] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for my_minizinc_image, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="my_minizinc_image:latest"
Nov 02 11:29:11 minikube kubelet[1580]: E1102 11:29:11.306602    1580 kuberuntime_image.go:53] "Failed to pull image" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for my_minizinc_image, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="my_minizinc_image:latest"
Nov 02 11:29:11 minikube kubelet[1580]: E1102 11:29:11.306657    1580 kuberuntime_manager.go:1212] container &Container{Name:minizinc-container,Image:my_minizinc_image,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{536870912 0} {<nil>}  BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ctw5j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},} start failed in pod minizinc-deployment-85594f856c-pghq7_mys(7b335bb8-36f0-4e5a-bbe2-e855e25d95a1): ErrImagePull: rpc error: code = Unknown desc = Error response from daemon: pull access denied for my_minizinc_image, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
Nov 02 11:29:11 minikube kubelet[1580]: E1102 11:29:11.306684    1580 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"minizinc-container\" with ErrImagePull: \"rpc error: code = Unknown desc = Error response from daemon: pull access denied for my_minizinc_image, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="mys/minizinc-deployment-85594f856c-pghq7" podUID=7b335bb8-36f0-4e5a-bbe2-e855e25d95a1
Nov 02 11:29:17 minikube kubelet[1580]: E1102 11:29:17.811330    1580 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"my-frontend\" with ImagePullBackOff: \"Back-off pulling image \\\"my-frontend\\\"\"" pod="default/my-frontend-7c9f66848f-s68vg" podUID=570da166-fa49-4dbe-ab98-3cbf2bc0a96f
Nov 02 11:29:18 minikube kubelet[1580]: E1102 11:29:18.813049    1580 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"minizinc-container\" with ImagePullBackOff: \"Back-off pulling image \\\"my_minizinc_image:latest\\\"\"" pod="default/minizinc-deployment-748c66865f-qc758" podUID=138eb270-69fe-4b31-a50f-b9cd0d8e5c08
Nov 02 11:29:19 minikube kubelet[1580]: E1102 11:29:19.811775    1580 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"my-backend\" with ImagePullBackOff: \"Back-off pulling image \\\"my-backend\\\"\"" pod="default/my-backend-6fc58fc9c6-rx7fj" podUID=1495c692-fbfc-4dc2-90e2-944f36c041e8
Nov 02 11:29:22 minikube kubelet[1580]: E1102 11:29:22.811018    1580 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"my-frontend\" with ImagePullBackOff: \"Back-off pulling image \\\"my-frontend\\\"\"" pod="default/my-frontend-5f6bccb46d-fbfts" podUID=3041c1f0-a64e-4d5e-bb1c-0669ce07321c
Nov 02 11:29:23 minikube kubelet[1580]: E1102 11:29:23.811761    1580 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"my-backend\" with ImagePullBackOff: \"Back-off pulling image \\\"my-backend\\\"\"" pod="default/my-backend-647678857b-mrj45" podUID=0e00cdb9-097b-45a4-8f91-3ecd81ef8e98
Nov 02 11:29:26 minikube kubelet[1580]: E1102 11:29:26.811533    1580 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"minizinc-container\" with ImagePullBackOff: \"Back-off pulling image \\\"my_minizinc_image\\\"\"" pod="mys/minizinc-deployment-85594f856c-pghq7" podUID=7b335bb8-36f0-4e5a-bbe2-e855e25d95a1
Nov 02 11:29:31 minikube kubelet[1580]: E1102 11:29:31.811732    1580 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"minizinc-container\" with ImagePullBackOff: \"Back-off pulling image \\\"my_minizinc_image:latest\\\"\"" pod="default/minizinc-deployment-748c66865f-qc758" podUID=138eb270-69fe-4b31-a50f-b9cd0d8e5c08
Nov 02 11:29:33 minikube kubelet[1580]: E1102 11:29:33.811073    1580 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"my-frontend\" with ImagePullBackOff: \"Back-off pulling image \\\"my-frontend\\\"\"" pod="default/my-frontend-5f6bccb46d-fbfts" podUID=3041c1f0-a64e-4d5e-bb1c-0669ce07321c
Nov 02 11:29:34 minikube kubelet[1580]: E1102 11:29:34.304735    1580 remote_image.go:167] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for my-frontend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="my-frontend:latest"
Nov 02 11:29:34 minikube kubelet[1580]: E1102 11:29:34.304763    1580 kuberuntime_image.go:53] "Failed to pull image" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for my-frontend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="my-frontend:latest"
Nov 02 11:29:34 minikube kubelet[1580]: E1102 11:29:34.304817    1580 kuberuntime_manager.go:1212] container &Container{Name:my-frontend,Image:my-frontend,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:3000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hk79j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},} start failed in pod my-frontend-7c9f66848f-s68vg_default(570da166-fa49-4dbe-ab98-3cbf2bc0a96f): ErrImagePull: rpc error: code = Unknown desc = Error response from daemon: pull access denied for my-frontend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
Nov 02 11:29:34 minikube kubelet[1580]: E1102 11:29:34.304843    1580 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"my-frontend\" with ErrImagePull: \"rpc error: code = Unknown desc = Error response from daemon: pull access denied for my-frontend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/my-frontend-7c9f66848f-s68vg" podUID=570da166-fa49-4dbe-ab98-3cbf2bc0a96f
Nov 02 11:29:34 minikube kubelet[1580]: E1102 11:29:34.811264    1580 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"my-backend\" with ImagePullBackOff: \"Back-off pulling image \\\"my-backend\\\"\"" pod="default/my-backend-6fc58fc9c6-rx7fj" podUID=1495c692-fbfc-4dc2-90e2-944f36c041e8
Nov 02 11:29:36 minikube kubelet[1580]: E1102 11:29:36.812032    1580 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"my-backend\" with ImagePullBackOff: \"Back-off pulling image \\\"my-backend\\\"\"" pod="default/my-backend-647678857b-mrj45" podUID=0e00cdb9-097b-45a4-8f91-3ecd81ef8e98
Nov 02 11:29:41 minikube kubelet[1580]: E1102 11:29:41.811299    1580 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"minizinc-container\" with ImagePullBackOff: \"Back-off pulling image \\\"my_minizinc_image\\\"\"" pod="mys/minizinc-deployment-85594f856c-pghq7" podUID=7b335bb8-36f0-4e5a-bbe2-e855e25d95a1
Nov 02 11:29:44 minikube kubelet[1580]: E1102 11:29:44.811691    1580 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"minizinc-container\" with ImagePullBackOff: \"Back-off pulling image \\\"my_minizinc_image:latest\\\"\"" pod="default/minizinc-deployment-748c66865f-qc758" podUID=138eb270-69fe-4b31-a50f-b9cd0d8e5c08
Nov 02 11:29:46 minikube kubelet[1580]: E1102 11:29:46.811410    1580 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"my-frontend\" with ImagePullBackOff: \"Back-off pulling image \\\"my-frontend\\\"\"" pod="default/my-frontend-7c9f66848f-s68vg" podUID=570da166-fa49-4dbe-ab98-3cbf2bc0a96f
Nov 02 11:29:47 minikube kubelet[1580]: E1102 11:29:47.257144    1580 remote_image.go:167] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for my-backend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="my-backend:latest"
Nov 02 11:29:47 minikube kubelet[1580]: E1102 11:29:47.257174    1580 kuberuntime_image.go:53] "Failed to pull image" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for my-backend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="my-backend:latest"
Nov 02 11:29:47 minikube kubelet[1580]: E1102 11:29:47.257229    1580 kuberuntime_manager.go:1212] container &Container{Name:my-backend,Image:my-backend,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:5000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m6fzz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},} start failed in pod my-backend-6fc58fc9c6-rx7fj_default(1495c692-fbfc-4dc2-90e2-944f36c041e8): ErrImagePull: rpc error: code = Unknown desc = Error response from daemon: pull access denied for my-backend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
Nov 02 11:29:47 minikube kubelet[1580]: E1102 11:29:47.257254    1580 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"my-backend\" with ErrImagePull: \"rpc error: code = Unknown desc = Error response from daemon: pull access denied for my-backend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/my-backend-6fc58fc9c6-rx7fj" podUID=1495c692-fbfc-4dc2-90e2-944f36c041e8
Nov 02 11:29:50 minikube kubelet[1580]: E1102 11:29:50.279177    1580 remote_image.go:167] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for my-frontend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="my-frontend:latest"
Nov 02 11:29:50 minikube kubelet[1580]: E1102 11:29:50.279206    1580 kuberuntime_image.go:53] "Failed to pull image" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for my-frontend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="my-frontend:latest"
Nov 02 11:29:50 minikube kubelet[1580]: E1102 11:29:50.279338    1580 kuberuntime_manager.go:1212] container &Container{Name:my-frontend,Image:my-frontend,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:3000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dgj8n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},} start failed in pod my-frontend-5f6bccb46d-fbfts_default(3041c1f0-a64e-4d5e-bb1c-0669ce07321c): ErrImagePull: rpc error: code = Unknown desc = Error response from daemon: pull access denied for my-frontend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
Nov 02 11:29:50 minikube kubelet[1580]: E1102 11:29:50.279377    1580 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"my-frontend\" with ErrImagePull: \"rpc error: code = Unknown desc = Error response from daemon: pull access denied for my-frontend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/my-frontend-5f6bccb46d-fbfts" podUID=3041c1f0-a64e-4d5e-bb1c-0669ce07321c
Nov 02 11:29:51 minikube kubelet[1580]: E1102 11:29:51.771394    1580 remote_image.go:167] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for my-backend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="my-backend:latest"
Nov 02 11:29:51 minikube kubelet[1580]: E1102 11:29:51.771420    1580 kuberuntime_image.go:53] "Failed to pull image" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for my-backend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="my-backend:latest"
Nov 02 11:29:51 minikube kubelet[1580]: E1102 11:29:51.771469    1580 kuberuntime_manager.go:1212] container &Container{Name:my-backend,Image:my-backend,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:5000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dlkxp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},} start failed in pod my-backend-647678857b-mrj45_default(0e00cdb9-097b-45a4-8f91-3ecd81ef8e98): ErrImagePull: rpc error: code = Unknown desc = Error response from daemon: pull access denied for my-backend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
Nov 02 11:29:51 minikube kubelet[1580]: E1102 11:29:51.771493    1580 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"my-backend\" with ErrImagePull: \"rpc error: code = Unknown desc = Error response from daemon: pull access denied for my-backend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/my-backend-647678857b-mrj45" podUID=0e00cdb9-097b-45a4-8f91-3ecd81ef8e98
Nov 02 11:29:55 minikube kubelet[1580]: E1102 11:29:55.811571    1580 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"minizinc-container\" with ImagePullBackOff: \"Back-off pulling image \\\"my_minizinc_image:latest\\\"\"" pod="default/minizinc-deployment-748c66865f-qc758" podUID=138eb270-69fe-4b31-a50f-b9cd0d8e5c08
Nov 02 11:29:56 minikube kubelet[1580]: E1102 11:29:56.811682    1580 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"minizinc-container\" with ImagePullBackOff: \"Back-off pulling image \\\"my_minizinc_image\\\"\"" pod="mys/minizinc-deployment-85594f856c-pghq7" podUID=7b335bb8-36f0-4e5a-bbe2-e855e25d95a1
Nov 02 11:29:58 minikube kubelet[1580]: E1102 11:29:58.812330    1580 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"my-frontend\" with ImagePullBackOff: \"Back-off pulling image \\\"my-frontend\\\"\"" pod="default/my-frontend-7c9f66848f-s68vg" podUID=570da166-fa49-4dbe-ab98-3cbf2bc0a96f
Nov 02 11:29:59 minikube kubelet[1580]: E1102 11:29:59.811264    1580 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"my-backend\" with ImagePullBackOff: \"Back-off pulling image \\\"my-backend\\\"\"" pod="default/my-backend-6fc58fc9c6-rx7fj" podUID=1495c692-fbfc-4dc2-90e2-944f36c041e8
Nov 02 11:30:04 minikube kubelet[1580]: E1102 11:30:04.811397    1580 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"my-frontend\" with ImagePullBackOff: \"Back-off pulling image \\\"my-frontend\\\"\"" pod="default/my-frontend-5f6bccb46d-fbfts" podUID=3041c1f0-a64e-4d5e-bb1c-0669ce07321c
Nov 02 11:30:05 minikube kubelet[1580]: E1102 11:30:05.811554    1580 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"my-backend\" with ImagePullBackOff: \"Back-off pulling image \\\"my-backend\\\"\"" pod="default/my-backend-647678857b-mrj45" podUID=0e00cdb9-097b-45a4-8f91-3ecd81ef8e98
Nov 02 11:30:06 minikube kubelet[1580]: E1102 11:30:06.811560    1580 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"minizinc-container\" with ImagePullBackOff: \"Back-off pulling image \\\"my_minizinc_image:latest\\\"\"" pod="default/minizinc-deployment-748c66865f-qc758" podUID=138eb270-69fe-4b31-a50f-b9cd0d8e5c08
Nov 02 11:30:07 minikube kubelet[1580]: E1102 11:30:07.811389    1580 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"minizinc-container\" with ImagePullBackOff: \"Back-off pulling image \\\"my_minizinc_image\\\"\"" pod="mys/minizinc-deployment-85594f856c-pghq7" podUID=7b335bb8-36f0-4e5a-bbe2-e855e25d95a1

* 
* ==> storage-provisioner [2b1fed94a9da] <==
* I1102 10:52:29.556056       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F1102 10:52:59.567744       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: i/o timeout

* 
* ==> storage-provisioner [c2c1a278071f] <==
* I1102 10:53:12.894555       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I1102 10:53:12.904489       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I1102 10:53:12.905198       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I1102 10:53:30.294057       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I1102 10:53:30.294136       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_9dbf5446-0d82-4e31-99ad-f377e1fee24b!
I1102 10:53:30.294201       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"a6c74b08-5e0a-4cb5-904c-cf02dfd43044", APIVersion:"v1", ResourceVersion:"19780", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_9dbf5446-0d82-4e31-99ad-f377e1fee24b became leader
I1102 10:53:30.395329       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_9dbf5446-0d82-4e31-99ad-f377e1fee24b!

